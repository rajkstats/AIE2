{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctrwj6Cj24Zp"
      },
      "source": [
        "# LangChain with Open Source LLM and Open Source Embeddings & LangSmith\n",
        "\n",
        "In the following notebook we will dive into the world of Open Source models hosted on Hugging Face's [inference endpoints](https://ui.endpoints.huggingface.co/).\n",
        "\n",
        "The notebook will be broken into the following parts:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Set-up Hugging Face Infrence Endpoints\n",
        "  2. Install required libraries\n",
        "  3. Set Environment Variables\n",
        "  4. Testing our Hugging Face Inference Endpoint\n",
        "  5. Creating LangChain components powered by the endpoints\n",
        "  6. Retrieving data from Arxiv\n",
        "  7. Creating a simple RAG pipeline with [LangChain v0.1.0](https://blog.langchain.dev/langchain-v0-1-0/)\n",
        "  \n",
        "\n",
        "- ü§ù Breakout Room #2:\n",
        "  1. Set-up LangSmith\n",
        "  2. Creating a LangSmith dataset\n",
        "  3. Creating a custom evaluator\n",
        "  4. Initializing our evaluator config\n",
        "  5. Evaluating our RAG pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AduTna3oCbP4"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENUY6OSnDy7A"
      },
      "source": [
        "## Task 1: Set-up Hugging Face Infrence Endpoints\n",
        "\n",
        "Please follow the instructions provided [here](https://github.com/AI-Maker-Space/AI-Engineering-Cohort-2/tree/main/Week%205/Day%202) to set-up your Hugging Face inference endpoints for both your LLM and your Embedding Models.\n",
        "\n",
        "- Open-source LLM: **NousResearch/Llama-2-7b-chat-hf**\n",
        "- Open-source Embedding Model:  **Snowflake/snowflake-arctic-embed-m**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-spIWt2J3Quk"
      },
      "source": [
        "## Task 2: Install required libraries\n",
        "\n",
        "Now we've got to get our required libraries!\n",
        "\n",
        "We'll start with our `langchain` and `huggingface` dependencies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EwGLnp31jXJj"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-core langchain-community langchain_openai huggingface-hub requests -q -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPXElql-EE9Q"
      },
      "source": [
        "Now we can grab some miscellaneous dependencies that will help us power our RAG pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FMJqq8SYt34V"
      },
      "outputs": [],
      "source": [
        "!pip install arxiv pymupdf faiss-cpu -q -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LTFTplIvbht",
        "outputId": "9de8e659-8953-43a6-fa95-a457830278a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.21.6 in /usr/local/lib/python3.10/dist-packages (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.21.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpZTBLwK3TIz"
      },
      "source": [
        "## Task 3: Set Environment Variables\n",
        "\n",
        "We'll need to set our `HF_TOKEN` so that we can send requests to our protected API endpoint.\n",
        "\n",
        "We'll also set-up our OpenAI API key, which we'll leverage later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NspG8I0XlFTt",
        "outputId": "566adf61-e8e9-4b7f-d818-854b04da6c5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HuggingFace Write Token: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"HuggingFace Write Token: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giMejsXN7EKb",
        "outputId": "894c5950-f774-4a53-ad8f-4e008919395c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3M7TzXs3WsJ"
      },
      "source": [
        "## Task 4: Testing our Hugging Face Inference Endpoint\n",
        "\n",
        "Let's submit a sample request to the Hugging Face Inference endpoint!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We're using **NousResearch/Llama-2-7b-chat-hf** as our HF inference endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uyFgZVUSEexW"
      },
      "outputs": [],
      "source": [
        "model_api_gateway = \"https://w00ju9fjxc2vzppr.us-east-1.aws.endpoints.huggingface.cloud\" # << YOUR ENDPOINT URL HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvnMlmEsEiqS"
      },
      "source": [
        "> NOTE: If you're running into issues finding your API URL you can find it at [this](https://ui.endpoints.huggingface.co/) link.\n",
        "\n",
        "Here's an example:\n",
        "\n",
        "![image](https://i.imgur.com/XyZhOv8.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fVaR1onmtkz",
        "outputId": "7df1f74f-4e12-42af-c8b6-fd4bc33d4c1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': \"Hello! How are you? I'm doing well, thanks for asking! *smiling*\\n\\n\\n\"}]\n"
          ]
        }
      ],
      "source": [
        "import requests  # Import the requests library to make HTTP requests\n",
        "\n",
        "# Define key parameters for the language model generation\n",
        "max_new_tokens = 256  # The maximum number of tokens (words/pieces of words) to generate\n",
        "top_p = 0.9  # The \"nucleus sampling\" parameter, selecting the top p% most likely next tokens\n",
        "temperature = 0.1  # The randomness of the response; closer to 0 makes it more deterministic\n",
        "\n",
        "# The text prompt we are sending to the model\n",
        "prompt = \"Hello! How are you?\"\n",
        "\n",
        "# Forming the JSON body to be sent to the API\n",
        "json_body = {\n",
        "    \"inputs\": prompt,  # The input text prompt\n",
        "    \"parameters\": {  # Parameters to guide the text generation\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"top_p\": top_p,\n",
        "        \"temperature\": temperature\n",
        "    }\n",
        "}\n",
        "\n",
        "# Headers including authorization and content type\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\",  # Access token for API authentication\n",
        "    \"Content-Type\": \"application/json\"  # Specify the content type of the request\n",
        "}\n",
        "\n",
        "# The URL of the model API endpoint (not shown in this snippet)\n",
        "model_api_gateway = 'https://api.example.com/model'\n",
        "\n",
        "# Making a POST request to the API with the JSON body and headers\n",
        "response = requests.post(model_api_gateway, json=json_body, headers=headers)\n",
        "\n",
        "# Printing the JSON response from the API\n",
        "print(response.json())  # This will show the generated text or any error messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXTBnBTy3b62"
      },
      "source": [
        "## Task 5: Creating LangChain components powered by the endpoints\n",
        "\n",
        "We're going to wrap our endpoints in LangChain components in order to leverage them, thanks to LCEL, as we would any other LCEL component!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd5DaxGEFohF"
      },
      "source": [
        "### HuggingFaceEndpoint for LLM\n",
        "\n",
        "We can use the `HuggingFaceEndpoint` found [here](https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/llms/huggingface_endpoint.py) to power our chain - let's look at how we would implement it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vc7K1rFhSVt",
        "outputId": "e1001b47-7aab-4634-b90e-16e2de222f37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import HuggingFaceEndpoint\n",
        "\n",
        "endpoint_url = (\n",
        "    model_api_gateway\n",
        ")\n",
        "\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=endpoint_url,\n",
        "    huggingfacehub_api_token=os.environ[\"HF_TOKEN\"],\n",
        "    task=\"text-generation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-PBb3MPFN_t"
      },
      "source": [
        "Now we can use our endpoint like we would any other LLM!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "mMJrWnKISFqb",
        "outputId": "b99da97d-824e-48dd-e031-a230d8ede8d4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\nYes, I am a big fan of your work and I must say that I am very impressed by the way you have been able to blend different styles and create something unique and interesting. Your use of color and light is particularly impressive and it really adds depth and emotion to your work.\\n\\nI was wondering if you could tell me a little bit about your creative process and how you come up with your ideas? Do you have any specific techniques or rituals that you use to get into a creative state of mind?\\n\\nAlso, I was curious to know if you have any upcoming exhibitions or projects that you are working on? I would love to hear more about them.\\n\\nThank you so much for your time and I look forward to hearing back from you.\\n\\nBest regards,\\n[Your Name]'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hf_llm.invoke(\"Hello, how are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EBtSBMj3-Hu"
      },
      "source": [
        "### HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "Now we can leverage the `HuggingFaceInferenceAPIEmbeddings` module in LangChain to connect to our Hugging Face Inference Endpoint hosted embedding model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll be using **Snowflake/snowflake-arctic-embed-m** for our embedding model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wrZJHVGkGLZr"
      },
      "outputs": [],
      "source": [
        "embedding_api_gateway = \"https://drk3aaxtilds5moy.us-east-1.aws.endpoints.huggingface.cloud\" # << Embedding Endpoint API URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4asz9Ofn0MtP"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "embeddings_model = HuggingFaceInferenceAPIEmbeddings(api_key=os.environ[\"HF_TOKEN\"], api_url=embedding_api_gateway)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvF_eMZZKnlm",
        "outputId": "75e494c3-451c-4c93-82db-6cde627e9178"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[-0.026445465,\n",
              " 0.03584882,\n",
              " 0.009444279,\n",
              " 0.011648565,\n",
              " 0.006504414,\n",
              " 0.008241854,\n",
              " -0.036926128,\n",
              " -0.036308434,\n",
              " -0.02483537,\n",
              " -0.0053980113]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings_model.embed_query(\"Hello, welcome to HF Endpoint Embeddings\")[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtbNzDF-e7JI"
      },
      "source": [
        "#### ‚ùì Question #1\n",
        "\n",
        "What is the embedding dimension of your selected embeddings model?\n",
        "\n",
        "\n",
        "#### ANSWER\n",
        "We are using [snowflake-arctic-embed-m](https://huggingface.co/Snowflake/snowflake-arctic-embed-m) as our embedding model and corresponding embedding dimension is 768"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9pLgHfR3uY9"
      },
      "source": [
        "## Task 6: Retrieving data from Arxiv\n",
        "\n",
        "We'll leverage the `ArxivLoader` to load some papers about the \"QLoRA\" topic, and then split them into more manageable chunks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7yO05R6mtyCB"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import ArxivLoader\n",
        "\n",
        "docs = ArxivLoader(query=\"QLoRA\", load_max_docs=5).load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4F249yWeuCKd"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 250,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "split_chunks = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9BO1Y1Xur0e",
        "outputId": "466fcfd8-f2f7-4f22-8d14-ff61a311b206"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1305"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sZBBjdM4Or8"
      },
      "source": [
        "Just the same as we would with OpenAI's embeddings model - we can instantiate our `FAISS` vector store with our documents and our `HuggingFaceEmbeddings` model!\n",
        "\n",
        "We'll need to take a few extra steps, though, due to a few limitations of the endpoint/FAISS.\n",
        "\n",
        "We'll start by embeddings our documents in batches of `32`.\n",
        "\n",
        "> NOTE: This process might take a while depending on the compute you assigned your embedding endpoint!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FBCTm-JZ0mVr"
      },
      "outputs": [],
      "source": [
        "embeddings = []\n",
        "for i in range(0, len(split_chunks) - 1, 32):\n",
        "  embeddings.append(embeddings_model.embed_documents([document.page_content for document in split_chunks[i:i+32]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4wLY8FDGNDym"
      },
      "outputs": [],
      "source": [
        "embeddings = [item for sub_list in embeddings for item in sub_list]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgc_e-9QHJTm"
      },
      "source": [
        "#### ‚ùì Question #2\n",
        "\n",
        "Why do we have to limit our batches when sending to the Hugging Face endpoints?\n",
        "\n",
        "#### ANSWER\n",
        "- Reason to limit batch sizes when sending requests to the Hugging Face Inference Endpoints is to avoid running out of memory on the underlying infrastructure as large models can consume significant amount of GPU memory\n",
        "- If you send requests with batch sizes that are too large for the available memory, you may encounter errors like \"503 Service Unavailable\" as the system tries to load the model\n",
        "- Starting with smaller batch sizes and gradually increasing them, rather than jumping to a very large batch size\n",
        "\n",
        "Reference: https://huggingface.co/docs/api-inference/en/faq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn4lECg2TTza"
      },
      "source": [
        "Now we can create text/embedding pairs which we want use to set-up our FAISS VectorStore!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UW3dmvnyUZB",
        "outputId": "93efa5c6-0a93-411d-c67b-b1106e9ad3dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to convert embedding for document: Dettmers and Zettlemoyer [13] and measure post-quantization zero-shot accuracy and perplexity\n",
            "across different models (OPT [72], LLaMA [57], BLOOM [52], Pythia [7]) for model sizes 125m -\n",
            "Failed to convert embedding for document: 13B (11.3 GB)\n",
            "33B (24.7 GB)\n",
            "65B (45.0 GB)\n",
            "Input gradient\n",
            "Optimizer\n",
            "Weight gradient\n",
            "Adapters\n",
            "Model\n",
            "Figure 6: Breakdown of the memory footprint of different LLaMA models. The input gradient size is for batch\n",
            "Failed to convert embedding for document: size 1 and sequence length 512 and is estimated only for adapters and the base model weights (no attention).\n",
            "Numbers on the bars are memory footprint in MB of individual elements of the total footprint. While some\n",
            "Failed to convert embedding for document: models do not quite fit on certain GPUs, paged optimzier provide enough memory to make these models fit.\n",
            "G\n",
            "Memory Footprint\n",
            "The memory footpring for QLoRA training with different LLaMA base models can be seen in\n",
            "Failed to convert embedding for document: Figure 6. We see that the 33B model does not quite fit into a 24 GB and that paged optimizers\n",
            "are needed to train it. Depicted is also batch size 1 with a sequence length of 512 and gradient\n",
            "Failed to convert embedding for document: checkpointning. This means, if one uses a larger batch size, or if a long sequence is processed, the\n",
            "activation gradient might consume a considerable amount of memory.\n",
            "Failed to convert embedding for document: Table 13: The complete ordering induced by pairwise GPT-4 judgments between systems\n",
            "Model\n",
            "Params\n",
            "Size\n",
            "Guanaco\n",
            "65B\n",
            "41 GB\n",
            "Guanaco\n",
            "33B\n",
            "21 GB\n",
            "Vicuna\n",
            "13B\n",
            "26 GB\n",
            "ChatGPT-3.5 Turbo\n",
            "N/A\n",
            "N/A\n",
            "Bard\n",
            "N/A\n",
            "N/A\n",
            "Guanaco\n",
            "13B\n",
            "10 GB\n",
            "Guanaco\n",
            "7B\n",
            "5 GB\n",
            "26\n",
            "Failed to convert embedding for document: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\n",
            "Haotong Qin * 1 2 Xudong Ma * 1 Xingyu Zheng 1 Xiaoyang Li 3 Yang Zhang 3 Shouda Liu 3 Jie Luo 1\n",
            "Xianglong LiuB 1 Michele Magno 2\n",
            "Abstract\n",
            "Failed to convert embedding for document: LLaMA2 (Touvron et al., 2023b) families (7B, 13B, 30B,\n",
            "and 65B), and constructs parameter-efficient finetuning on\n",
            "Alpaca (Taori et al., 2023) and Flan v2 (Longpre et al.,\n",
            "2023) datasets. The Massively Multitask Language Under-\n",
            "Failed to convert embedding for document: standing (MMLU) (Hendrycks et al., 2020) and Common-\n",
            "senseQA benchmarks(e.g.HellaSwag (Zellers et al., 2019),\n",
            "5\n",
            "Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\n",
            "Table 1: Accuracy (%) comparison of LLaMA on the\n",
            "Failed to convert embedding for document: MMLU finetuned on the Alpaca dataset\n",
            "Method\n",
            "#Bit\n",
            "MMLU\n",
            "Hums. STEM Social Other Avg.\n",
            "LLaMA-7B\n",
            "16\n",
            "33.3\n",
            "29.8\n",
            "37.8\n",
            "38.0\n",
            "34.6\n",
            "PEQA\n",
            "4\n",
            "34.9\n",
            "28.9\n",
            "37.5\n",
            "40.1\n",
            "34.8\n",
            "NormalFloat\n",
            "4\n",
            "33.1\n",
            "30.6\n",
            "38.8\n",
            "38.8\n",
            "35.1\n",
            "QLoRA w/ GPTQ\n",
            "4\n",
            "33.8\n",
            "31.3\n",
            "37.4\n",
            "42.2\n",
            "36.0\n",
            "QLoRA\n",
            "4\n",
            "36.1\n",
            "31.9\n",
            "Failed to convert embedding for document: 42.0\n",
            "44.5\n",
            "38.4\n",
            "QA-LoRA\n",
            "4\n",
            "36.6\n",
            "32.4\n",
            "44.8\n",
            "44.9\n",
            "39.4\n",
            "IR-QLoRA (ours)\n",
            "4\n",
            "38.6\n",
            "34.6\n",
            "45.2\n",
            "45.5\n",
            "40.8\n",
            "LLaMA-13B\n",
            "16\n",
            "40.6\n",
            "36.7\n",
            "48.9\n",
            "48.0\n",
            "43.3\n",
            "NormalFloat\n",
            "4\n",
            "43.0\n",
            "34.5\n",
            "51.8\n",
            "51.4\n",
            "45.0\n",
            "PEQA\n",
            "4\n",
            "43.0\n",
            "37.7\n",
            "53.6\n",
            "49.0\n",
            "45.0\n",
            "QLoRA\n",
            "4\n",
            "45.4\n",
            "37.4\n",
            "55.7\n",
            "54.3\n",
            "48.0\n",
            "QLoRA w/ GPTQ\n",
            "Failed to convert embedding for document: 4\n",
            "48.4\n",
            "38.3\n",
            "54.9\n",
            "55.2\n",
            "49.2\n",
            "QA-LoRA\n",
            "4\n",
            "48.4\n",
            "38.3\n",
            "54.9\n",
            "55.2\n",
            "49.2\n",
            "IR-QLoRA (ours)\n",
            "4\n",
            "47.2\n",
            "39.0\n",
            "56.5\n",
            "55.0\n",
            "49.3\n",
            "LLaMA-30B\n",
            "16\n",
            "56.2\n",
            "45.9\n",
            "67.1\n",
            "63.9\n",
            "58.2\n",
            "NormalFloat\n",
            "4\n",
            "55.3\n",
            "44.7\n",
            "66.2\n",
            "63.3\n",
            "57.3\n",
            "QLoRA\n",
            "4\n",
            "55.4\n",
            "46.0\n",
            "66.4\n",
            "63.6\n",
            "57.7\n",
            "QLoRA w/ GPTQ\n",
            "4\n",
            "55.8\n",
            "46.4\n",
            "67.0\n",
            "Failed to convert embedding for document: 64.0\n",
            "58.1\n",
            "QA-LoRA\n",
            "4\n",
            "55.8\n",
            "46.4\n",
            "67.0\n",
            "64.0\n",
            "58.1\n",
            "IR-QLoRA (ours)\n",
            "4\n",
            "56.7\n",
            "46.7\n",
            "66.5\n",
            "63.2\n",
            "58.2\n",
            "LLaMA-65B\n",
            "16\n",
            "61.4\n",
            "51.9\n",
            "73.6\n",
            "67.6\n",
            "63.4\n",
            "QA-LoRA\n",
            "4\n",
            "60.8\n",
            "50.5\n",
            "72.5\n",
            "66.7\n",
            "62.5\n",
            "NormalFloat\n",
            "4\n",
            "60.7\n",
            "52.3\n",
            "72.6\n",
            "67.3\n",
            "63.0\n",
            "QLoRA w/ GPTQ\n",
            "4\n",
            "60.4\n",
            "52.5\n",
            "73.0\n",
            "67.2\n",
            "63.0\n",
            "QLoRA\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# List to hold valid text and embedding pairs\n",
        "valid_text_embedding_pairs = []\n",
        "\n",
        "# Iterate through the documents and their corresponding embeddings\n",
        "for document, emb in zip(split_chunks, embeddings):\n",
        "    try:\n",
        "        # Ensure embedding is a numpy array and convert it to float32\n",
        "        emb_array = np.array(emb, dtype=np.float32)\n",
        "        # Append the valid document content and embedding to the list\n",
        "        valid_text_embedding_pairs.append((document.page_content, emb_array))\n",
        "    except ValueError:\n",
        "        # Handle cases where conversion to float array fails\n",
        "        print(f\"Failed to convert embedding for document: {document.page_content}\")\n",
        "        # Optionally, handle the error by continuing, logging, or using a placeholder embedding\n",
        "        # Example of using a placeholder:\n",
        "        # placeholder_embedding = np.zeros_like(valid_text_embedding_pairs[0][1])  # Assuming at least one valid embedding exists\n",
        "        # valid_text_embedding_pairs.append((document.page_content, placeholder_embedding))\n",
        "\n",
        "# Check if there are any valid embeddings left\n",
        "if not valid_text_embedding_pairs:\n",
        "    raise ValueError(\"No valid embeddings available.\")\n",
        "\n",
        "# Create the FAISS vector store with the valid pairs\n",
        "faiss_vectorstore = FAISS.from_embeddings(valid_text_embedding_pairs, embeddings_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXbexmFSTZKF"
      },
      "source": [
        "Next, we set up FAISS as a retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BSUZYfvAPxTF"
      },
      "outputs": [],
      "source": [
        "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\" : 5})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce1ZWj8aTchK"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DwHoaIDQQ9E",
        "outputId": "35983ebf-185a-4add-d65e-8b0ff38e9c89"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[Document(page_content='We have discussed how QLoRA works and how it can significantly reduce the required memory for\\nfinetuning models. The main question now is whether QLoRA can perform as well as full-model'),\n",
              " Document(page_content='performance degradation. Our method, QLORA, uses a novel high-precision technique to quantize\\na pretrained model to 4-bit, then adds a small set of learnable Low-rank Adapter weights [28]\\n‚àóEqual contribution.'),\n",
              " Document(page_content='wFP16 | w) denotes the conditional entropy of ÀÜ\\nwFP16\\ngiven w. As deterministic quantizers are used in the quan-\\ntization of LLMs, H( ÀÜ\\nwFP16 | w) = 0 and the I( ÀÜ\\nwFP16; w)\\ndepends on H( ÀÜ\\nwFP16) directly. In the PTQ, since the origi-'),\n",
              " Document(page_content='A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Language Processing, pages 2369‚Äì2380, 2018.'),\n",
              " Document(page_content='ders information recovery. LoRA mitigates the performance\\ndegradation caused by weight quantization in LLM by fine-\\ntuning additional adapters for downstream tasks, sometimes\\neven yielding notable performance enhancements. The fine-')]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "faiss_retriever.get_relevant_documents(\"What optimizer does QLoRA use?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm0IjkpFSdmw"
      },
      "source": [
        "### Prompt Template\n",
        "\n",
        "Now that we have our LLM and our Retiever set-up, let's connect them with our Prompt Template!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Gqpayd-kTyiq"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT_TEMPLATE = \"\"\"\\\n",
        "Using the provided context, please answer the user's question. If you don't know, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NikHqHljIIdK"
      },
      "source": [
        "#### ‚ùì Question #3\n",
        "\n",
        "Does the ordering of the prompt matter?\n",
        "\n",
        "#### ANSWER\n",
        "\n",
        "Yes, ordering of the prompt matters. Both placing the prompt before or after the text can be effective in guiding GPT to perform the desired task\n",
        "- **Contextual understanding**: Placing the prompt before the text allows the model to immediately understand the task and frame its response accordingly. This can help ensure that the model‚Äôs understanding of the task is clear from the beginning.\n",
        "- **Prompt length:** Placing the prompt after the text could potentially lead to longer overall input, especially if the input text is lengthy. In such cases, placing the prompt before the text may be advantageous as it allows for a shorter and more concise prompt.\n",
        "- **Task clarity:** Placing the prompt before the text provides a clear instruction to the model about the intended task. It sets the context and expectation for the response.\n",
        "\n",
        "Reference: https://community.openai.com/t/when-processing-a-text-prompt-before-it-or-after-it/247801/3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwy1YOy34aXf"
      },
      "source": [
        "## Task 7: Creating a simple RAG pipeline with LangChain v0.1.0\n",
        "\n",
        "All that's left to do is set up a RAG chain - and away we go!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "i0q8CUu809M-"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | faiss_retriever, # Retrieve relevant context using FAISS based on the input question.\n",
        "        \"question\": itemgetter(\"question\"), # Extract the question from the input data for use in the next steps\n",
        "    }\n",
        "    | rag_prompt # Generate a prompt integrating the retrieved context and the original question for the language model.\n",
        "    | hf_llm # Query a Hugging Face large language model with the generated prompt to produce an answer.\n",
        "    | StrOutputParser() # Parse the output from the language model into a structured format.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHyy5p484iUD"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "OezUhZGrUr63",
        "outputId": "38b7bf4c-d82a-4948-99ae-a51625eb44cb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nAnswer:\\nQLoRA is a method for reducing the memory required for finetuning large language models. It does this by transforming the weights of the model into a lower-rank matrix, which can be stored in less memory. The method has been shown to be effective in reducing the memory required for finetuning, but it is not clear how well it performs compared to full-model finetuning.'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLORA all about?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Some metrics from HF Endpoints (LLM and Embedding model)\n",
        "\n",
        "HF LLM endpoint metrics\n",
        "![image](https://i.imgur.com/OGz0bf4.png)\n",
        "\n",
        "HF Embedding model endpoint metrics\n",
        "![image](https://i.imgur.com/9iLUv3G.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGsV8x_ZIWZ9"
      },
      "source": [
        "# ü§ù Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrKQSs_r4gl8"
      },
      "source": [
        "## Task 1: Set-up LangSmith\n",
        "\n",
        "We'll be moving through this notebook to explain what visibility tools can do to help us!\n",
        "\n",
        "Technically, all we need to do is set-up the next cell's environment variables!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S5X3EE847PO",
        "outputId": "6fe39294-26ef-4ace-b530-bf06aaffbd98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your LangSmith API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE2 - {unique_id}\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou1fLN-MJGfu"
      },
      "source": [
        "Let's see what happens on the LangSmith project when we run this chain now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "1Yr8j5hqJGET",
        "outputId": "fc8518e5-06b6-4149-9dc7-8dfd148b2c96"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nAnswer:\\nQLoRA is a method for quantizing low-rank approximations of neural network weights. It is designed to reduce the memory required for finetuning pre-trained models, and can perform as well as full-model quantization.'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLoRA all about?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmaxEfcWJWXc"
      },
      "source": [
        "We get *all of this information* for \"free\":\n",
        "\n",
        "![image](https://i.imgur.com/EptDE8G.png)\n",
        "\n",
        "\n",
        "> NOTE: We'll walk through this diagram in detail in class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsFaAg1TJ8JE"
      },
      "source": [
        "####üèóÔ∏è Activity #1:\n",
        "\n",
        "Please describe the trace of the previous request and answer these questions:\n",
        "\n",
        "1. How many tokens did the request use?\n",
        "2. How long did the `HuggingFaceEndpoint` take to complete?\n",
        "\n",
        "#### ANSWER\n",
        "- Request used 402 tokens (353 -prompt tokens and 49 completion tokens)\n",
        "- `HuggingFaceEndpoint` took 4.55s to complete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XdbE0m3JgJp"
      },
      "source": [
        "## Task 2: Creating a LangSmith dataset\n",
        "\n",
        "Now that we've got LangSmith set-up - let's explore how we can create a dataset!\n",
        "\n",
        "First, we'll create a list of questions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "-KVSO6Eh5DpC"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urLbc0B8K6QZ"
      },
      "source": [
        "Now we can create our dataset through the LangSmith `Client()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "NUH0m7AuKyn7"
      },
      "outputs": [],
      "source": [
        "client = Client()\n",
        "dataset_name = \"QLoRA RAG Dataset v2\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    dataset_id=dataset.id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jxaByg9LFfX"
      },
      "source": [
        "After this step you should be able to navigate to the following dataset in the LangSmith web UI.\n",
        "\n",
        "![image](https://i.imgur.com/2QyvYMQ.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbVQaJi3LsdU"
      },
      "source": [
        "## Task 3: Creating a custom evaluator\n",
        "\n",
        "Now that we have a dataset - we can start thinking about evaluation.\n",
        "\n",
        "We're going to make a `StringEvaluator` to measure \"dopeness\".\n",
        "\n",
        "> NOTE: While this is a fun toy example - this can be extended to practically any use-case!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "qofRv8FI7TeZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Any, Optional\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.evaluation import StringEvaluator\n",
        "\n",
        "class DopenessEvaluator(StringEvaluator):\n",
        "    \"\"\"An LLM-based dopeness evaluator.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "\n",
        "        template = \"\"\"On a scale from 0 to 100, how dope (cool, awesome, lit) is the following response to the input:\n",
        "        --------\n",
        "        INPUT: {input}\n",
        "        --------\n",
        "        OUTPUT: {prediction}\n",
        "        --------\n",
        "        Reason step by step about why the score is appropriate, then print the score at the end. At the end, repeat that score alone on a new line.\"\"\"\n",
        "\n",
        "        self.eval_chain = PromptTemplate.from_template(template) | llm\n",
        "\n",
        "    @property\n",
        "    def requires_input(self) -> bool:\n",
        "        return True\n",
        "\n",
        "    @property\n",
        "    def requires_reference(self) -> bool:\n",
        "        return False\n",
        "\n",
        "    @property\n",
        "    def evaluation_name(self) -> str:\n",
        "        return \"scored_dopeness\"\n",
        "\n",
        "    def _evaluate_strings(\n",
        "        self,\n",
        "        prediction: str,\n",
        "        input: Optional[str] = None,\n",
        "        reference: Optional[str] = None,\n",
        "        **kwargs: Any\n",
        "    ) -> dict:\n",
        "        evaluator_result = self.eval_chain.invoke(\n",
        "            {\"input\": input, \"prediction\": prediction}, kwargs\n",
        "        )\n",
        "        reasoning, score = evaluator_result.content.split(\"\\n\", maxsplit=1)\n",
        "        score = re.search(r\"\\d+\", score).group(0)\n",
        "        if score is not None:\n",
        "            score = float(score.strip()) / 100.0\n",
        "        return {\"score\": score, \"reasoning\": reasoning.strip()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PoETszTMSNW"
      },
      "source": [
        "## Task 4: Initializing our evaluator config\n",
        "\n",
        "Now we can initialize our `RunEvalConfig` which we can use to evaluate our chain against our dataset.\n",
        "\n",
        "> NOTE: Check out the [documentation](https://docs.smith.langchain.com/evaluation/faq/custom-evaluators) for adding additional custom evaluators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "pc0bedbe-S2z"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset  # Importing necessary classes from the langchain.smith module\n",
        "\n",
        "# Configuration for running evaluations on a dataset\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[DopenessEvaluator()],  # List of custom evaluator instances; DopenessEvaluator must be defined elsewhere\n",
        "    evaluators=[\n",
        "        \"criteria\",  # Using built-in criteria evaluator\n",
        "        RunEvalConfig.Criteria(\"harmfulness\"),  # Evaluating based on harmfulness criterion\n",
        "        RunEvalConfig.Criteria(\n",
        "            {\n",
        "                \"AI\": \"Does the response feel AI generated?\"  # Custom criterion with a description\n",
        "                \"Response Y if they do, and N if they don't.\"  # Instructions for the evaluation (seems to be missing a comma at the end of the previous line)\n",
        "            }\n",
        "        ),\n",
        "    ],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XalvsOjMvdK"
      },
      "source": [
        "## Task 5: Evaluating our RAG pipeline\n",
        "\n",
        "All that's left to do now is evaluate our pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6syFWlaF-olk",
        "outputId": "fb0822a7-1b30-45e5-a263-d701fced14e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'HF RAG Pipeline - Evaluation - v1' at:\n",
            "https://smith.langchain.com/o/bbdaa341-a469-5436-ba9e-24733ea4fe6d/datasets/2bb9ebca-0ccc-4802-a664-20aa1bd62426/compare?selectedSessions=ca07b9f1-7fe5-4fa7-ab6f-7853eab0bef4\n",
            "\n",
            "View all tests for Dataset QLoRA RAG Dataset v2 at:\n",
            "https://smith.langchain.com/o/bbdaa341-a469-5436-ba9e-24733ea4fe6d/datasets/2bb9ebca-0ccc-4802-a664-20aa1bd62426\n",
            "[------------------------------------------------->] 6/6\n",
            " Experiment Results:\n",
            "        feedback.helpfulness  feedback.harmfulness  feedback.AI  feedback.scored_dopeness error  execution_time                                run_id\n",
            "count                   6.00                  6.00         6.00                      6.00     0            6.00                                     6\n",
            "unique                   NaN                   NaN          NaN                       NaN     0             NaN                                     6\n",
            "top                      NaN                   NaN          NaN                       NaN   NaN             NaN  ec6b9145-cedd-4a2d-8195-a4deae8eba9d\n",
            "freq                     NaN                   NaN          NaN                       NaN   NaN             NaN                                     1\n",
            "mean                    0.17                  0.00         0.33                      0.34   NaN            6.00                                   NaN\n",
            "std                     0.41                  0.00         0.52                      0.34   NaN            2.54                                   NaN\n",
            "min                     0.00                  0.00         0.00                      0.10   NaN            3.60                                   NaN\n",
            "25%                     0.00                  0.00         0.00                      0.10   NaN            4.24                                   NaN\n",
            "50%                     0.00                  0.00         0.00                      0.15   NaN            5.98                                   NaN\n",
            "75%                     0.00                  0.00         0.75                      0.57   NaN            6.12                                   NaN\n",
            "max                     1.00                  0.00         1.00                      0.85   NaN           10.59                                   NaN\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'HF RAG Pipeline - Evaluation - v1',\n",
              " 'results': {'135b4bee-07a7-407d-9038-2408270201e9': {'input': {'question': 'What optimizer is used in QLoRA?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate. \\n\\nLooking at the submission, the respondent simply states \"I don\\'t know.\" This does not provide any helpful or insightful information about the question asked, which is about the optimizer used in QLoRA. \\n\\nThe response is appropriate in the sense that it is a valid response to a question, but it does not meet the criteria of being helpful or insightful. \\n\\nTherefore, the submission does not meet the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('0b3c1681-8cfc-4824-9586-bb57efd603db'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is about whether the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is clear that the respondent does not know the answer to the question. This is not harmful, offensive, or inappropriate. It is simply an admission of lack of knowledge. \\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('ed9ef8ea-bc5e-4b08-a91e-dd77f67f3baf'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is to assess whether the response feels like it was generated by an AI. The submission is \"I don\\'t know.\" This response could be generated by both a human and an AI, as it is a common phrase used when the answer to a question is unknown. However, it does not contain any specific AI-like characteristics such as overly formal language, lack of context, or inability to understand the question. Therefore, it does not particularly feel AI-generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('8a712a52-b813-45f4-a117-0ab8c6cb2a73'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.1, value=None, comment=\"The response is not very dope because it doesn't provide any useful information. The purpose of asking a question is to get an answer, and in this case, the response failed to deliver. It's not cool or awesome to not know something, especially when the purpose of the interaction is to provide information. Therefore, the score is quite low.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('62a3e538-2905-4c1f-a5cf-99b0fbbc3fe7'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 3.603228,\n",
              "   'run_id': 'ec6b9145-cedd-4a2d-8195-a4deae8eba9d',\n",
              "   'output': \"\\nAnswer:\\nI don't know.\"},\n",
              "  '4b80ca4e-a2ef-4dbb-9b89-822af58fc3dd': {'input': {'question': 'What data type was created in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". To determine if the submission is helpful, we need to assess if it provides a clear, accurate, and relevant response to the input.\\n\\nThe input asks for the data type created in the QLoRA paper. The submission provides a response, stating that the data type is \"4-bit\". \\n\\nHowever, without context or further information, it\\'s difficult to determine if this answer is accurate or relevant. The QLoRA paper could be about a variety of topics, and \"4-bit\" could refer to a number of different data types. \\n\\nTherefore, based on the information provided, it\\'s unclear if the submission is helpful. \\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('bd8f1b41-86c1-48d6-99bc-937ac577a694'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion asks if the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is a simple, factual response to a question about a specific data type created in a paper. \\n\\nThere is no language or content that could be considered harmful, offensive, or inappropriate. \\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('754bc793-f26d-4bf4-8543-96914e789360'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is asking whether the response feels like it was generated by an AI. The response provided is straightforward and factual, answering the question directly. It does not exhibit any of the common signs of AI-generated text, such as nonsensical phrases, irrelevant information, or unusual sentence structures. Therefore, it does not feel AI-generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('302ac79c-0223-4398-b77f-d15b270f4c3b'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.85, value=None, comment=\"The response is accurate and directly answers the question asked. It doesn't include any unnecessary information or jargon, making it easy to understand. However, it lacks a bit of context or explanation about what the 4-bit data type is or why it was created in the QLoRA paper, which could have made the response more informative and engaging.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('651960e5-9d9e-42fd-badb-bb8716e26112'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 6.152996,\n",
              "   'run_id': '0f934e69-8a09-4cb6-aee1-c520f80d6a81',\n",
              "   'output': '\\nAnswer:\\nThe data type created in the QLoRA paper is 4-bit.'},\n",
              "  '130a7412-ed76-4d01-a96f-3ae564a335fa': {'input': {'question': 'What is a Retrieval Augmented Generation system?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is helpfulness. The submission should be helpful, insightful, and appropriate.\\n\\nLooking at the submission, the answer provided is \"I don\\'t know.\" This is not helpful or insightful as it does not provide any information about the input question. The explanation also does not provide any additional information or insight. It simply states that the term is not mentioned in the provided context, which is not relevant as the question is asking for a definition or explanation of the term, not its presence in a context.\\n\\nTherefore, the submission does not meet the criterion of being helpful, insightful, and appropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('65aae796-fc08-41e9-8a40-0f42baf19407'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion asks if the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, the answer provided is \"I don\\'t know.\" This is not harmful, offensive, or inappropriate. It may not be helpful or informative, but it does not meet the criteria for harmfulness.\\n\\nThe explanation provided is \"The term \"Retrieval Augmented Generation system\" is not mentioned in the provided context.\" This is also not harmful, offensive, or inappropriate. It is a factual statement about the lack of context provided.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('3aad4a5c-0ad8-4334-a3ac-c05f546c54b1'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=1, value='Y', comment='The criterion is to assess whether the response feels like it was generated by an AI. The submission consists of an answer and an explanation. The answer is \"I don\\'t know\" which could be generated by both a human or an AI. The explanation states that the term \"Retrieval Augmented Generation system\" is not mentioned in the provided context. This is a logical response that could be generated by an AI, as it is based on the absence of information in the provided context. Therefore, the response could be perceived as AI-generated.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('dcf99759-e0c5-4eda-a7c5-ffd2c8cba964'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.1, value=None, comment='This response is not very dope at all. The AI system is supposed to provide information about the query, but instead, it simply states that it doesn\\'t know. This is not helpful or informative. Furthermore, the explanation is incorrect. The term \"Retrieval Augmented Generation system\" doesn\\'t need to be mentioned in the provided context for the AI to provide an explanation. The AI should be able to provide information about the term regardless of the context. Therefore, the score is very low.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('017d335e-0f73-41da-b0ad-f2a93ea7be06'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 6.023089,\n",
              "   'run_id': '28b4d55f-dbba-4bb9-827e-052b94538cf2',\n",
              "   'output': '\\nAnswer:\\nI don\\'t know.\\n\\nExplanation:\\nThe term \"Retrieval Augmented Generation system\" is not mentioned in the provided context.'},\n",
              "  '08deb760-a50a-4a38-81c2-e4d5af022c71': {'input': {'question': 'Who authored the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". The submission should provide a helpful, insightful, and appropriate response to the input.\\n\\nLooking at the input, the question asks for the author of the QLoRA paper. This is a specific question that requires a specific answer.\\n\\nThe submission, however, does not provide the specific answer to the question. Instead, it asks for more context or clarification. While this might be seen as an attempt to be helpful, it does not directly answer the question. \\n\\nTherefore, based on the criterion of helpfulness, the submission does not meet the criteria because it does not provide a direct answer to the question.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('294dee35-3f00-48d8-b109-b31dc69389ca'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion asks if the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is a polite request for more information to accurately answer the question. The user is not sure about the author of the QLoRA paper and asks for more context or clarification. \\n\\nThere is no harmful, offensive, or inappropriate content in the submission. \\n\\nTherefore, the submission meets the criterion. \\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('97a02825-e702-43bd-95ab-85212685d62e'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=1, value='Y', comment='The criterion is to assess whether the response feels AI generated or not. The submission is a response to a question about the author of a specific paper. The response is unsure and asks for more context or clarification. This kind of response could be generated by an AI, as it does not provide a specific answer but rather asks for more information, which is a common strategy used by AI when the input is not clear or lacks sufficient information. Therefore, the response does feel AI generated.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('db48284d-131a-483a-a73b-6b1d526f590c'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.2, value=None, comment=\"The response is not dope at all because it does not provide the correct information. The QLoRA paper is a well-known paper in the field of technology and the assistant should be able to provide the authors' names. The assistant's inability to provide the correct information reduces its coolness factor. However, the assistant's polite request for more information and its clear communication style slightly increase its dope score.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('1f0d9211-94b9-43b8-9f6f-c9950bac8b35'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 5.934549,\n",
              "   'run_id': 'cf464e29-00b5-46c0-b13e-b1675ee05461',\n",
              "   'output': \"\\nAnswer:\\nI'm not sure. The paper you provided does not mention the author's name directly. Could you provide more context or clarify which paper you are referring to?\"},\n",
              "  '753e5bea-4172-4fb1-bf10-b32a46416fe5': {'input': {'question': 'What is the most popular deep learning framework?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate. \\n\\nLooking at the submission, the answer to the question \"What is the most popular deep learning framework?\" is \"I don\\'t know.\" \\n\\nThis answer is not helpful because it does not provide any information that would help the person asking the question. It is also not insightful because it does not provide any new or interesting information. \\n\\nThe answer could be considered appropriate in the sense that it is a valid response to a question, but in the context of this task, where the goal is to provide a helpful and insightful answer, it is not appropriate. \\n\\nTherefore, the submission does not meet the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('59cfef24-97c0-4c59-8ba7-ca13c9fa9418'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is asking whether the submission is harmful, offensive, or inappropriate. \\n\\nThe submission is \"I don\\'t know.\" \\n\\nThis submission is not harmful as it does not cause any damage or injury. \\n\\nIt is also not offensive as it does not insult or cause any discomfort. \\n\\nLastly, it is not inappropriate as it does not violate any norms or rules. \\n\\nTherefore, the submission is not harmful, offensive, or inappropriate. \\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('3d74e3d1-b623-4186-b985-04dee7373687'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is to assess whether the response feels like it was generated by an AI. The response provided is \"I don\\'t know.\" This is a simple and straightforward response that could be given by both a human and an AI. However, considering the context of the question, an AI trained in the field of deep learning should be able to provide the name of the most popular deep learning framework. Therefore, the response does not feel AI generated as it lacks the expected knowledge an AI in this field should possess.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('c4599918-ecc0-4265-a46a-262061110793'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.1, value=None, comment=\"The response is not very dope. It's not providing any useful or interesting information, and it's not engaging or entertaining. It's just a simple statement of ignorance, which isn't very cool or awesome. It's also not lit, because it's not exciting or impressive. It's just a basic, straightforward response that doesn't add anything to the conversation.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('e631c611-8220-4f57-9d37-750565622bea'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 3.668573,\n",
              "   'run_id': 'da4c0d45-696f-40be-8bc1-4df323c57e14',\n",
              "   'output': \"\\nAnswer: I don't know.\"},\n",
              "  'f8e7abec-83a7-4f1c-a545-c7b10687e0a1': {'input': {'question': 'What significant improvements does the LoRA system make?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is the helpfulness of the submission. \\n\\nLooking at the submission, it provides a detailed explanation of the improvements made by the LoRA system. It mentions the reduction in memory requirement, the use of more adapters, optimization of the loss function, and the trade-off between accurate and efficient search. It also compares the LoRA system with other methods, providing a context for understanding its improvements. \\n\\nThe submission is insightful as it not only states the improvements but also explains how they are achieved. It provides specific details like the setting of the coefficient Œª and the number of candidates œÉ, which adds depth to the answer.\\n\\nThe submission is appropriate as it directly answers the input question about the significant improvements made by the LoRA system. It stays on topic and provides relevant information.\\n\\nBased on this analysis, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('ddebd7bc-06b9-4edc-8230-72418c42c5a4'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this assessment is whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a technical explanation of the improvements made by the LoRA system. It does not contain any harmful, offensive, or inappropriate content. It is purely factual and informative.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('7dba975c-d5b9-4c58-88c9-3caed606ab50'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion asks if the response feels like it was generated by an AI. The response provided is coherent, detailed, and accurate. It uses technical language appropriately and provides a comprehensive answer to the question. It does not exhibit typical signs of AI-generated text, such as nonsensical phrases, irrelevant information, or awkward sentence structure. Therefore, it does not feel AI-generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('29e20aeb-6192-4c3d-a693-fbda257366fd'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.7, value=None, comment='This response is quite informative and detailed, providing a clear explanation of the improvements made by the LoRA system. It uses technical language appropriately and accurately, demonstrating a good understanding of the topic. However, it might be a bit too technical for some people to understand, and it doesn\\'t have a particularly exciting or engaging tone. It\\'s a solid, professional response, but it\\'s not particularly \"dope\" or \"lit\".', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a82465e6-2298-411f-b7f9-24443a618346'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 10.594776,\n",
              "   'run_id': '245b273e-b4d1-4c1d-a839-c2079878859f',\n",
              "   'output': '\\nAnswer:\\nThe LoRA system makes significant improvements in terms of reducing the memory requirement of the model during training. The system uses more adapters to improve the memory footprint of the model, and it also optimizes the loss function for each candidate. The LoRA system also achieves a trade-off between accurate and efficient search by setting the coefficient Œª and the number of candidates œÉ to achieve the best results. Additionally, the LoRA system performs superior to other methods, such as QLoRA, for smaller-size models, and it achieves better results for larger models as well.'}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Execute an evaluation run on a specific dataset using a pre-configured client\n",
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,  # Name of the dataset to use for the evaluation\n",
        "    llm_or_chain_factory=retrieval_augmented_qa_chain,  # The language model or processing chain to be used for answering queries\n",
        "    evaluation=eval_config,  # Evaluation configuration as defined previously, includes custom and built-in evaluators\n",
        "    verbose=True,  # Enables verbose output to provide detailed logs during the execution\n",
        "    project_name=\"HF RAG Pipeline - Evaluation - v1\",  # A descriptive name for the project, useful for logging and tracking purposes\n",
        "    project_metadata={\"version\": \"1.0.0\"},  # Additional metadata for the project, useful for version control\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
