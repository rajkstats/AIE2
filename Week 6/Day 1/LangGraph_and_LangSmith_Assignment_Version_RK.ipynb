{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ðŸ¤ Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "  \n",
        "- ðŸ¤ Breakout Room #2:\n",
        "  - Part 1:\n",
        "    1. Creating an Evaluation Dataset\n",
        "    2. Adding Evaluators\n",
        "    3. Evaluating\n",
        "  - Part 2:\n",
        "    1. Adding conditional check."
      ],
      "metadata": {
        "id": "gJXW_DgiSebM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¤ Breakout Room #1"
      ],
      "metadata": {
        "id": "djQ3nRAgoF67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effetively allowing us to recreate application flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ],
      "metadata": {
        "id": "e7pQDUhUnIo8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1:  Dependencies\n",
        "\n",
        "We'll first install all our required libraries."
      ],
      "metadata": {
        "id": "3_fLDElOVoop"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "KaVwN269EttM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da8294f8-4276-48ad-e654-1b97eae59585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/2.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain langchain_openai langgraph arxiv duckduckgo-search tavily-python"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ],
      "metadata": {
        "id": "wujPjGJuoPwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "085523ba-15bb-40ac-a324-fa044e0a3613"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8COGBrCXKHSN",
        "outputId": "fd64564b-2b27-4d9c-fec7-4882ab552e5e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE2 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "21946fb9-8071-4ea4-ca33-89b393f49282"
      },
      "execution_count": 45,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangSmith API Key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ],
      "metadata": {
        "id": "sBRyQmEAVzua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####ðŸ—ï¸ Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ],
      "metadata": {
        "id": "2k6n_Dob2F46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tool_belt = [\n",
        "    TavilySearchResults(),\n",
        "    ArxivQueryRun()\n",
        "]"
      ],
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actioning with Tools\n",
        "\n",
        "Now that we've created our tool belt - we need to create a process that will let us leverage them when we need them.\n",
        "\n",
        "We'll use the built-in [`ToolExecutor`](https://github.com/langchain-ai/langgraph/blob/main/langgraph/prebuilt/tool_executor.py) to do so."
      ],
      "metadata": {
        "id": "1FdOjEslXdRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import ToolExecutor\n",
        "\n",
        "tool_executor = ToolExecutor(tool_belt)"
      ],
      "metadata": {
        "id": "cFr1m80-JZsD"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ],
      "metadata": {
        "id": "VI-C669ZYVI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ],
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "\n",
        "functions = [convert_to_openai_function(t) for t in tool_belt]\n",
        "model = model.bind_functions(functions)"
      ],
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### â“ Question #1:\n",
        "\n",
        "How does the model determine which tool to use?\n",
        "\n",
        "#### ANSWER\n",
        "- Model uses its natural language understanding capabilities to map the query to the most appropriate tool function based on the tool descriptions and capabilities.\n",
        "- Binding of tool functions to the model enables it to directly invoke the tools as part of its response generation process"
      ],
      "metadata": {
        "id": "ERzuGo6W18Lr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ],
      "metadata": {
        "id": "_296Ub96Z_H8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ],
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ],
      "metadata": {
        "id": "vWsMhfO9grLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import FunctionMessage\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "def call_tool(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  action = ToolInvocation(\n",
        "      tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
        "      tool_input=json.loads(\n",
        "          last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
        "      )\n",
        "  )\n",
        "\n",
        "  response = tool_executor.invoke(action)\n",
        "\n",
        "  function_message = FunctionMessage(content=str(response), name=action.tool)\n",
        "\n",
        "  return {\"messages\" : [function_message]}"
      ],
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `call_tool` is a node which will call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ],
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", call_tool)"
      ],
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ],
      "metadata": {
        "id": "b8CjRlbVmRpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ],
      "metadata": {
        "id": "uaXHpPeSnOWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workflow.set_entry_point(\"agent\")"
      ],
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ],
      "metadata": {
        "id": "BUsfGoSpoF9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ],
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# determine whether the workflow should continue or end based on the state of the graph\n",
        "# Retrieves the last message from the state dictionary\n",
        "# checks if function call is present, if not present then there are no more actions to take\n",
        "# returns \"continue\" to indicate that the workflow should continue to the next action\n",
        "\n",
        "# Conditional Edge\n",
        "# agent represents the starting node of the conditional edge\n",
        "# should_continue function, which will be called to determine the next node based on the state\n",
        "# If should_continue returns \"continue\", the workflow will proceed to the \"action\" node\n",
        "# If should_continue returns \"end\", the workflow will proceed to the END node, indicating the end of the workflow\n",
        "\n",
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if \"function_call\" not in last_message.additional_kwargs:\n",
        "    return \"end\"\n",
        "\n",
        "  return \"continue\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ],
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ],
      "metadata": {
        "id": "yKCjWJCkrJb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workflow.add_edge(\"action\", \"agent\")"
      ],
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ],
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ],
      "metadata": {
        "id": "KYqDpErlsCsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### â“ Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?\n",
        "\n",
        "#### ANSWER\n",
        "- There is no specific limit imposed on the number of cycles or iterations that the LangGraph workflow can go through by default.\n",
        "- We can impose a limit to number of cycles by modifying **should_continue** function by adding a counter variable and check if it has reached a predefined limit. If limit is reached, then end the loop"
      ],
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helper Function to print messages"
      ],
      "metadata": {
        "id": "GSCds6zTL5VJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_messages(messages):\n",
        "  next_is_tool = False\n",
        "  initial_query = True\n",
        "  for message in messages[\"messages\"]:\n",
        "    if \"function_call\" in message.additional_kwargs:\n",
        "      print()\n",
        "      print(f'Tool Call - Name: {message.additional_kwargs[\"function_call\"][\"name\"]} + Query: {message.additional_kwargs[\"function_call\"][\"arguments\"]}')\n",
        "      next_is_tool = True\n",
        "      continue\n",
        "    if next_is_tool:\n",
        "      print(f\"Tool Response: {message.content}\")\n",
        "      next_is_tool = False\n",
        "      continue\n",
        "    if initial_query:\n",
        "      print(f\"Initial Query: {message.content}\")\n",
        "      print()\n",
        "      initial_query = False\n",
        "      continue\n",
        "    print()\n",
        "    print(f\"Agent Response: {message.content}\")\n"
      ],
      "metadata": {
        "id": "xRPF0X5iL8Bh"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ],
      "metadata": {
        "id": "VEYcTShCsPaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"What is RAG in the context of Large Language Models? When did it break onto the scene?\")]}\n",
        "\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "eb95a3a4-38a5-4e76-8fcd-0998ff986d67"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Query: What is RAG in the context of Large Language Models? When did it break onto the scene?\n",
            "\n",
            "\n",
            "Tool Call - Name: tavily_search_results_json + Query: {\"query\":\"RAG in Large Language Models\"}\n",
            "Tool Response: [{'url': 'https://arxiv.org/abs/2405.06211', 'content': \"Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, retrieval-augmented large language models have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the generation quality of LLMs.\"}, {'url': 'https://arxiv.org/abs/2312.10997', 'content': 'Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge ...'}, {'url': 'https://arxiv.org/abs/2309.01431', 'content': 'Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we ...'}, {'url': 'https://huggingface.co/docs/transformers/model_doc/rag', 'content': 'Attention mask post-processed from the retrieved documents and the question encoder input_ids by the\\nretriever.\\nquestion_encoder_last_hidden_state (tf.Tensor of shape (batch_size, sequence_length, hidden_size), optional) â€” Sequence of hidden states at the output of the last layer of the question encoder pooled output of the\\nmodel.\\nquestion_enc_hidden_states (tuple(tf.Tensor), optional, returned when output_hidden_states=True is passed or when config.output_hidden_states=True) â€” Attention mask post-processed from the retrieved documents and the question encoder input_ids by the\\nretriever.\\nquestion_encoder_last_hidden_state (tf.Tensor of shape (batch_size, sequence_length, hidden_size), optional) â€” Sequence of hidden states at the output of the last layer of the question encoder pooled output of the\\nmodel.\\nquestion_enc_hidden_states (tuple(tf.Tensor), optional, returned when output_hidden_states=True is passed or when config.output_hidden_states=True) â€” Attention mask post-processed from the retrieved documents and the question encoder input_ids by the\\nretriever.\\nquestion_encoder_last_hidden_state (tf.Tensor of shape (batch_size, sequence_length, hidden_size), optional) â€” Sequence of hidden states at the output of the last layer of the question encoder pooled output of the\\nmodel.\\nquestion_enc_hidden_states (tuple(tf.Tensor), optional, returned when output_hidden_states=True is passed or when config.output_hidden_states=True) â€” Transformers documentation\\nRAG\\nTransformers\\nand get access to the augmented documentation experience\\nto get started\\nRAG\\nOverview\\nRetrieval-augmented generation (â€œRAGâ€) models combine the powers of pretrained dense retrieval (DPR) and\\nsequence-to-sequence models. generator_enc_last_hidden_state (tf.Tensor of shape (batch_size, sequence_length, hidden_size), optional) â€” Sequence of hidden-states at the output of the last layer of the generator encoder of the model.\\ngenerator_enc_hidden_states (tuple(tf.Tensor), optional, returned when output_hidden_states=True is passed or when config.output_hidden_states=True) â€”'}, {'url': 'https://aws.amazon.com/what-is/retrieval-augmented-generation/', 'content': 'What is Retrieval-Augmented Generation?\\nRetrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. How can AWS support your\\xa0Retrieval-Augmented Generation requirements?\\nAmazon Bedrock is a fully-managed service that offers a choice of high-performing foundation modelsâ€”along with a broad set of capabilitiesâ€”to build generative AI applications while simplifying development and maintaining privacy and security. Augment the LLM prompt\\nNext, the RAG model augments the user input (or prompts) by adding the relevant retrieved data in context. How does Retrieval-Augmented Generation work?\\nWithout RAG, the LLM takes the user input and creates a response based on information it was trained onâ€”or what it already knows. What is the difference between Retrieval-Augmented Generation and semantic search?\\nSemantic search enhances RAG results for organizations wanting to add vast external knowledge sources to their LLM applications.'}]\n",
            "\n",
            "Agent Response: Retrieval-Augmented Generation (RAG) is a process in which large language models optimize their output by referencing authoritative knowledge bases outside of their training data sources before generating a response. RAG has emerged as a solution to challenges faced by Large Language Models (LLMs) such as hallucination, outdated knowledge, and non-transparent reasoning processes. It enhances the accuracy and credibility of the generation by incorporating knowledge from external databases.\n",
            "\n",
            "RAG broke onto the scene as a promising approach to mitigate the hallucination of large language models and improve the generation quality of LLMs. It combines the powers of pretrained dense retrieval models and sequence-to-sequence models to provide more comprehensive and accurate results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"function_call\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"function_call\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ],
      "metadata": {
        "id": "DBHnUtLSscRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\")]}\n",
        "\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "36ebb42c-5f18-4eb7-9ede-f1f10fc7c7f4"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Query: What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\n",
            "\n",
            "\n",
            "Tool Call - Name: arxiv + Query: {\"query\":\"QLoRA in Machine Learning\"}\n",
            "Tool Response: Published: 2023-05-23\n",
            "Title: QLoRA: Efficient Finetuning of Quantized LLMs\n",
            "Authors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\n",
            "Summary: We present QLoRA, an efficient finetuning approach that reduces memory usage\n",
            "enough to finetune a 65B parameter model on a single 48GB GPU while preserving\n",
            "full 16-bit finetuning task performance. QLoRA backpropagates gradients through\n",
            "a frozen, 4-bit quantized pretrained language model into Low Rank\n",
            "Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\n",
            "previous openly released models on the Vicuna benchmark, reaching 99.3% of the\n",
            "performance level of ChatGPT while only requiring 24 hours of finetuning on a\n",
            "single GPU. QLoRA introduces a number of innovations to save memory without\n",
            "sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\n",
            "information theoretically optimal for normally distributed weights (b) double\n",
            "quantization to reduce the average memory footprint by quantizing the\n",
            "quantization constants, and (c) paged optimziers to manage memory spikes. We\n",
            "use QLoRA to finetune more than 1,000 models, providing a detailed analysis of\n",
            "instruction following and chatbot performance across 8 instruction datasets,\n",
            "multiple model types (LLaMA, T5), and model scales that would be infeasible to\n",
            "run with regular finetuning (e.g. 33B and 65B parameter models). Our results\n",
            "show that QLoRA finetuning on a small high-quality dataset leads to\n",
            "state-of-the-art results, even when using smaller models than the previous\n",
            "SoTA. We provide a detailed analysis of chatbot performance based on both human\n",
            "and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\n",
            "alternative to human evaluation. Furthermore, we find that current chatbot\n",
            "benchmarks are not trustworthy to accurately evaluate the performance levels of\n",
            "chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\n",
            "ChatGPT. We release all of our models and code, including CUDA kernels for\n",
            "4-bit training.\n",
            "\n",
            "Published: 2023-12-31\n",
            "Title: Viz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI\n",
            "Authors: Dipankar Sarkar\n",
            "Summary: This paper aims to introduce and analyze the Viz system in a comprehensive\n",
            "way, a novel system architecture that integrates Quantized Low-Rank Adapters\n",
            "(QLoRA) to fine-tune large language models (LLM) within a legally compliant and\n",
            "resource efficient marketplace. Viz represents a significant contribution to\n",
            "the field of artificial intelligence, particularly in addressing the challenges\n",
            "of computational efficiency, legal compliance, and economic sustainability in\n",
            "the utilization and monetization of LLMs. The paper delineates the scholarly\n",
            "discourse and developments that have informed the creation of Viz, focusing\n",
            "primarily on the advancements in LLM models, copyright issues in AI training\n",
            "(NYT case, 2023), and the evolution of model fine-tuning techniques,\n",
            "particularly low-rank adapters and quantized low-rank adapters, to create a\n",
            "sustainable and economically compliant framework for LLM utilization. The\n",
            "economic model it proposes benefits content creators, AI developers, and\n",
            "end-users, delineating a harmonious integration of technology, economy, and\n",
            "law, offering a comprehensive solution to the complex challenges of today's AI\n",
            "landscape.\n",
            "\n",
            "Published: 2024-02-08\n",
            "Title: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\n",
            "Authors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\n",
            "Summary: The LoRA-finetuning quantization of LLMs has been extensively studied to\n",
            "obtain accurate yet compact LLMs for deployment on resource-constrained\n",
            "hardware. However, existing methods cause the quantized LLM to severely degrade\n",
            "and even fail to benefit from the finetuning of LoRA. This paper proposes a\n",
            "novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\n",
            "through information retention. The proposed IR-QLoRA mainly relies on two\n",
            "technolog\n",
            "\n",
            "Tool Call - Name: tavily_search_results_json + Query: {\"query\":\"Tim Dettmers bio\"}\n",
            "Tool Response: [{'url': 'https://timdettmers.com/about/', 'content': 'Tim Dettmers, Luke Zettlemoyer. [bib] 2018. Convolutional 2D Knowledge Graph Embeddings, Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel. AAAI2018. 2016. 8-Bit Approximations for Parallelism in Deep Learning, Tim Dettmers. ICLR2016.'}, {'url': 'https://www.cs.utexas.edu/events/faculty-candidate-talk-tim-dettmersuniversity-washington-accessible-foundation-models-systems', 'content': \"Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ...\"}, {'url': 'https://cs.nyu.edu/dynamic/about/news/colloquium/1344/', 'content': \"Speaker Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning.\"}, {'url': 'https://huggingface.co/timdettmers', 'content': '261 followers. Â·. 1 following. https://timdettmers.com. tim_dettmers. timdettmers. None yet. Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA. A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using transformers, accelerate and bitsandbytes.'}, {'url': 'https://scholar.google.com/citations?user=lHI3w5kAAAAJ', 'content': 'Tim Dettmers. University of Washington. Verified email at cs.washington.edu - Homepage. Deep Learning Natural Language Processing. Articles Cited by Public access Co-authors. Title. ... A Borzunov, M Ryabinin, T Dettmers, Q Lhoest, L Saulnier, M Diskin, ... NeurIPS 2021 Demonstration, 2022. 9: 2022:'}]\n",
            "\n",
            "Agent Response: The QLoRA paper in Machine Learning is titled \"QLoRA: Efficient Finetuning of Quantized LLMs\" by Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. The paper introduces QLoRA, an efficient finetuning approach that reduces memory usage to finetune a 65B parameter model on a single 48GB GPU while maintaining full 16-bit finetuning task performance. The paper presents innovations such as 4-bit NormalFloat (NF4) data type, double quantization, and paged optimizers to save memory without compromising performance.\n",
            "\n",
            "The first author of the QLoRA paper is Tim Dettmers. Tim Dettmers is a researcher who focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. He works on developing compression and networking algorithms and building systems for memory-efficient, fast, and cost-effective deep learning. You can find more information about Tim Dettmers on his [website](https://timdettmers.com/about/).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####ðŸ—ï¸ Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer.\n",
        "\n",
        "\n",
        "#### ANSWER\n",
        "- Agent received the initial query which was \"What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\"\n",
        "\n",
        "- To answer the first part of the query about QLoRA, the agent performed a search on arXiv using the query \"QLoRA in Machine Learning\". The search returned relevant papers titled \"QLoRA: Efficient Finetuning of Quantized LLMs\" by Tim Dettmers et al,....\n",
        "\n",
        "- To find information about the first author of the QLoRA paper, Tim Dettmers, the agent performed a search using the query \"Tim Dettmers bio\" with Tavily Search Results\n",
        "\n",
        "- Agent synthesized the information from the search results to provide a comprehensive answer to the initial query, covering the definition of QLoRA, the relevant technical paper, and the bio of the first author, Tim Dettmers"
      ],
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¤ Breakout Room #2"
      ],
      "metadata": {
        "id": "JQmrzYfrm1Dr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-processing for LangSmith"
      ],
      "metadata": {
        "id": "pV3XeFOT1Sar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ],
      "metadata": {
        "id": "wruQCuzewUuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | app | parse_output"
      ],
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_chain.invoke({\"question\" : \"What is RAG?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "52e7bd46-ad4e-4426-9507-9735fae0d211"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am unable to retrieve search results for RAG at the moment. Would you like me to try searching for something else?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ],
      "metadata": {
        "id": "f9UkCIqkpyZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####ðŸ—ï¸ Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ],
      "metadata": {
        "id": "VfMXF2KAsQxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]"
      ],
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ],
      "metadata": {
        "id": "z7QVFuAmsh7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ],
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### â“ Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or\n",
        "\n",
        "#### ANSWER\n",
        "\n",
        "- Correct answers are associated with the questions using the must_mention field in the answers list\n",
        "- It can be a  bit problematic as it act as a constraint or requirement for the correct answer. For example, it doesn't ensure whether answer is complete pr provides sufficient information\n",
        "- As the number of questions will grow with time, it becomes a manual effort to ensure that the must_mention terms accurately represent the correct answers"
      ],
      "metadata": {
        "id": "ciV73F9Q04w0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ],
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ],
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### â“ Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method.\n",
        "\n",
        "#### ANSWER\n",
        "- It relies on exact string match, so may miss cases where required phrases are present but in slightly different form\n",
        "- It does binary scoring whether present or not, doesn't capture the partial correctness of response\n",
        "\n",
        "We can improve this by doing following:\n",
        "- Allow for multiple sets of required phrases and consider the output correct if it matches any one of the sets\n",
        "- Assigning partial scores based on the proportion of required phrases present\n",
        "- Use fuzzy matching techniques like Levenshtein distance or cosine similarity to allow for minor variations in the phrases"
      ],
      "metadata": {
        "id": "PNtHORUh0jZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have created our custom evaluator - let's initialize our `RunEvalConfig` with it, and a few others:\n",
        "\n",
        "- `\"criteria\"` includes the default criteria which, in this case, means \"helpfulness\"\n",
        "- `\"cot_qa\"` includes a criteria that bases whether or not the answer is correct by utilizing a Chain of Thought prompt and the provided context to determine if the response is correct or not."
      ],
      "metadata": {
        "id": "mZ4DVSXl0BX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[must_mention],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        \"cot_qa\",\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "sL4-XcjytWsu"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ],
      "metadata": {
        "id": "r1RJr349zhv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=agent_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:8]}\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5TeCUUkuGld",
        "outputId": "3c8d7a79-1072-47e8-ba46-9ef54fb5da9a"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for project 'RAG Pipeline - Evaluation - f057d1b1' at:\n",
            "https://smith.langchain.com/o/bbdaa341-a469-5436-ba9e-24733ea4fe6d/datasets/8359e4f5-8816-4f71-8454-e03fad6598b1/compare?selectedSessions=78c6d751-08f2-4ebc-92f1-bc424da9d8c6\n",
            "\n",
            "View all tests for Dataset Retrieval Augmented Generation - Evaluation Dataset - 7ed84977 at:\n",
            "https://smith.langchain.com/o/bbdaa341-a469-5436-ba9e-24733ea4fe6d/datasets/8359e4f5-8816-4f71-8454-e03fad6598b1\n",
            "[------------------------------------------------->] 6/6\n",
            " Experiment Results:\n",
            "        feedback.helpfulness  feedback.COT Contextual Accuracy feedback.must_mention error  execution_time                                run_id\n",
            "count                   6.00                              6.00                     6     0            6.00                                     6\n",
            "unique                   NaN                               NaN                     2     0             NaN                                     6\n",
            "top                      NaN                               NaN                  True   NaN             NaN  3a856bf5-1da1-4584-b35f-c6314c91ef33\n",
            "freq                     NaN                               NaN                     3   NaN             NaN                                     1\n",
            "mean                    0.83                              0.67                   NaN   NaN            5.94                                   NaN\n",
            "std                     0.41                              0.52                   NaN   NaN            4.94                                   NaN\n",
            "min                     0.00                              0.00                   NaN   NaN            1.70                                   NaN\n",
            "25%                     1.00                              0.25                   NaN   NaN            3.58                                   NaN\n",
            "50%                     1.00                              1.00                   NaN   NaN            4.03                                   NaN\n",
            "75%                     1.00                              1.00                   NaN   NaN            6.30                                   NaN\n",
            "max                     1.00                              1.00                   NaN   NaN           15.39                                   NaN\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'project_name': 'RAG Pipeline - Evaluation - f057d1b1',\n",
              " 'results': {'34e2f6c1-4304-4b62-a27d-100d4c7a362a': {'input': {'question': 'What optimizer is used in QLoRA?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a direct answer to the question asked in the input, which is about the optimizer used in QLoRA. The answer is specific and includes additional information on how to access the optimizer, which could be useful for someone who is trying to use QLoRA. \\n\\nTherefore, the submission can be considered helpful, insightful, and appropriate. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('86fddd92-031e-49cc-bda8-4453a8120161'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=0, value='INCORRECT', comment=\"The context provided does not give any information about the optimizer used in QLoRA. The student's answer mentions a 'Paged Optimizer', but without any information in the context to confirm this, we cannot verify the accuracy of the student's answer. \\nGRADE: INCORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('8a5b2bcf-80df-4a5d-b97b-2281967fa45f'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('6028b199-f051-43a2-bd8a-37c6dfabcc2b'), target_run_id=None)],\n",
              "   'execution_time': 4.087729,\n",
              "   'run_id': '3a856bf5-1da1-4584-b35f-c6314c91ef33',\n",
              "   'output': 'The optimizer used in QLoRA is the Paged Optimizer, which can be accessed with the argument \"--optim paged_adamw_32bit\".',\n",
              "   'reference': {'must_mention': ['paged', 'optimizer']}},\n",
              "  'c88e6c17-bb72-49e2-82f9-9ad5e536d714': {'input': {'question': 'What data type was created in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a clear and direct answer to the question asked in the input. It not only names the new data type introduced in the QLoRA paper, but also provides additional information about its characteristics. This additional information could be helpful for someone who is not familiar with the QLoRA paper or the \"4-bit NormalFloat (NF4)\" data type. \\n\\nTherefore, the submission is helpful, insightful, and appropriate. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a035bf84-491d-4f45-8c1d-5352f7f4475c'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The context mentions 'NF4' and 'NormalFloat' as the data type created in the QLoRA paper. The student's answer also mentions '4-bit NormalFloat (NF4)' as the data type created in the QLoRA paper. The student's answer matches the context. Therefore, the student's answer is correct.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('559cc738-4e31-47bd-b4af-b42aa4261ee4'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('5782f666-1f14-466d-a3e8-d2c66d505bb2'), target_run_id=None)],\n",
              "   'execution_time': 3.452928,\n",
              "   'run_id': 'cf6e552f-3110-462e-80e0-224aeec4ba5b',\n",
              "   'output': 'The QLoRA paper introduced a new data type called \"4-bit NormalFloat (NF4)\" which is information theoretically optimal for normally distributed weights.',\n",
              "   'reference': {'must_mention': ['NF4', 'NormalFloat']}},\n",
              "  '63c77049-c2c8-4a50-811f-4a1561380625': {'input': {'question': 'What is a Retrieval Augmented Generation system?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is the helpfulness of the submission. \\n\\nThe submission provides a detailed explanation of what a Retrieval Augmented Generation (RAG) system is. It explains the two main components of a RAG system, the retrieval component and the text generation component, and how they work together to generate a response. \\n\\nThe submission also provides context on where RAG systems are commonly used, such as in question-answering systems and chatbots. This information is helpful for understanding the practical applications of RAG systems.\\n\\nThe submission is insightful as it explains how RAG systems can improve the quality of generated text and provide more accurate and relevant information to users. This insight helps to understand the benefits of using a RAG system.\\n\\nThe submission is appropriate as it directly answers the question and provides relevant information about RAG systems. \\n\\nTherefore, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a1ef23c2-65ae-4274-bba5-375c1a155372'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer provides a detailed explanation of what a Retrieval Augmented Generation (RAG) system is. The student correctly identifies that a RAG system is a type of natural language processing system that combines information retrieval and text generation techniques. The student also correctly explains the role of the retrieval component in a RAG system and how it contributes to generating a response. The student further explains the applications of RAG systems, which is accurate and relevant to the question. Therefore, the student's answer is factually accurate and does not contain any conflicting statements.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('b3778ac2-e239-4c14-a842-7179e0cbc271'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('a5d38d5f-ae9f-4c21-8fac-566e13d61d03'), target_run_id=None)],\n",
              "   'execution_time': 3.9655,\n",
              "   'run_id': 'd03361da-e267-43ac-85d6-45d6bea5ede2',\n",
              "   'output': 'A Retrieval Augmented Generation (RAG) system is a type of natural language processing system that combines information retrieval and text generation techniques. In a RAG system, a retrieval component is used to search for relevant information from a large corpus of text, and this retrieved information is then used to generate a coherent and informative response.\\n\\nThe retrieval component in a RAG system helps in providing context and relevant information to the text generation component, which then generates a response based on the retrieved information. This approach allows the system to produce more accurate and contextually relevant responses compared to traditional text generation systems.\\n\\nRAG systems are commonly used in question-answering systems, chatbots, and other applications where generating informative and contextually relevant responses is important. By combining retrieval and generation techniques, RAG systems can improve the quality of generated text and provide more accurate and relevant information to users.',\n",
              "   'reference': {'must_mention': ['ground', 'context']}},\n",
              "  '12a4aa72-014a-4160-a4d1-5a33323bb4c4': {'input': {'question': 'Who authored the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". \\n\\nThe input asks for the author of the QLoRA paper. The submission provides the authors of a paper, but it is the QDyLoRA paper, not the QLoRA paper as asked in the input. \\n\\nWhile the submission is insightful in providing the authors of a paper, it is not appropriate or helpful in answering the specific question asked in the input. \\n\\nTherefore, the submission does not meet the criteria. \\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('c4b365a4-fbae-4b75-80f6-95ba76e28f18'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=0, value='INCORRECT', comment=\"The student's answer does not match the context provided. The context states that the authors of the QLoRA paper are 'Tim' and 'Dettmers', but the student has listed a completely different set of authors. Therefore, the student's answer is not factually accurate.\\nGRADE: INCORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('0dea9095-4fc1-412a-93c3-20c2b275c2f8'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('ac3e7fef-7111-4f28-9090-a109eed69b2b'), target_run_id=None)],\n",
              "   'execution_time': 15.393983,\n",
              "   'run_id': '6e937f1f-5d32-460e-95fc-df8159b9be00',\n",
              "   'output': 'The QDyLoRA paper was authored by Hossein Rajabzadeh, Mojtaba Valipour, Tianshu Zhu, Marzieh Tahaei, Hyock Ju Kwon, Ali Ghodsi, Boxing Chen, and Mehdi Rezagholizadeh.',\n",
              "   'reference': {'must_mention': ['Tim', 'Dettmers']}},\n",
              "  '2a641940-5e57-4a13-ad9c-e7423b1956b0': {'input': {'question': 'What is the most popular deep learning framework?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is the helpfulness of the submission. \\n\\nThe submission provides a clear answer to the question, stating that TensorFlow is the most popular deep learning framework. \\n\\nIn addition to answering the question, the submission also provides additional information about TensorFlow, such as its origin, its use, and the ecosystem it provides. This information is helpful for someone who may not be familiar with TensorFlow or deep learning frameworks in general.\\n\\nThe submission also mentions other popular deep learning frameworks, which provides a broader context and could be helpful for someone looking for alternatives to TensorFlow.\\n\\nBased on these points, the submission can be considered helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('90860f04-7515-495f-aad4-af1890ace2d0'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer states that TensorFlow is the most popular deep learning framework. This aligns with the context provided, which lists TensorFlow as one of the popular deep learning frameworks. The student also provides additional information about TensorFlow, which does not contradict the context. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('78044e29-3010-4333-9b6f-32a18dfec55b'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('f7d85d01-6768-438b-bf17-01633c702521'), target_run_id=None)],\n",
              "   'execution_time': 1.701514,\n",
              "   'run_id': '09e1a818-7322-4ece-a6e4-513906eacb7a',\n",
              "   'output': 'The most popular deep learning framework is TensorFlow. TensorFlow is an open-source machine learning framework developed by Google that is widely used for building and training deep learning models. It provides a comprehensive ecosystem of tools, libraries, and community support for deep learning research and applications. Other popular deep learning frameworks include PyTorch, Keras, and Caffe.',\n",
              "   'reference': {'must_mention': ['PyTorch', 'TensorFlow']}},\n",
              "  'debd9253-6cca-48d7-93ec-253deada0b74': {'input': {'question': 'What significant improvements does the LoRA system make?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate.\\n\\nLooking at the submission, it provides a detailed explanation of the improvements that the LoRA system makes. It lists six key improvements and explains each one in detail. This makes the submission helpful as it provides the information needed to understand the improvements of the LoRA system.\\n\\nThe submission is also insightful. It not only lists the improvements but also explains why these improvements are significant. For example, it explains why the long range and low power consumption of the LoRA system are beneficial for IoT applications.\\n\\nThe submission is appropriate as it directly answers the question asked in the input. It stays on topic and provides relevant information.\\n\\nBased on this analysis, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('65eedc30-35c3-4e63-93b7-d4baad193c1a'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer provides a detailed explanation of the improvements that the LoRA system makes. The student mentions range, power consumption, cost, scalability, interference immunity, and flexibility as key improvements. The context provided does not give specific improvements to compare the student's answer to, but the student's answer is factually accurate and comprehensive in terms of the improvements that the LoRA system makes. Therefore, the student's answer is correct.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('b14ad495-2173-44ee-96ef-9f0dd11d4ba4'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('d071334d-3226-4efa-b94e-f736a16b5fe2'), target_run_id=None)],\n",
              "   'execution_time': 7.033851,\n",
              "   'run_id': '59789a2e-2a41-4413-ab97-50a8a3c89f95',\n",
              "   'output': \"The LoRA (Long Range) system is a low-power wide-area network (LPWAN) technology that offers significant improvements in terms of range, power consumption, and cost compared to traditional wireless communication technologies. Some of the key improvements of the LoRA system include:\\n\\n1. Long Range: LoRA technology enables communication over long distances, typically ranging from several kilometers to tens of kilometers in rural areas. This extended range makes it suitable for applications that require connectivity over large geographic areas.\\n\\n2. Low Power Consumption: LoRA devices are designed to operate on low power, allowing them to have long battery life. This makes LoRA ideal for applications that require devices to operate for extended periods without frequent battery replacements.\\n\\n3. Low Cost: LoRA technology is cost-effective compared to other wireless communication technologies, making it accessible for a wide range of applications. The low cost of LoRA devices and infrastructure makes it an attractive option for deploying large-scale IoT networks.\\n\\n4. Scalability: The LoRA system is highly scalable, allowing for the deployment of a large number of devices within a network. This scalability makes it suitable for IoT applications that require connectivity for a large number of devices spread across a wide area.\\n\\n5. Interference Immunity: LoRA uses spread spectrum modulation techniques to provide robust communication in the presence of interference and noise. This immunity to interference ensures reliable communication even in challenging environments.\\n\\n6. Flexibility: LoRA technology offers flexibility in terms of data rate, range, and coverage, allowing for customization based on the specific requirements of the application. This flexibility makes it versatile for a wide range of IoT use cases.\\n\\nOverall, the LoRA system's significant improvements in range, power consumption, cost, scalability, interference immunity, and flexibility make it a popular choice for IoT applications that require long-range, low-power communication.\",\n",
              "   'reference': {'must_mention': ['reduce', 'parameters']}}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2:\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add a custom node and conditional edge to determine if the response was helpful enough."
      ],
      "metadata": {
        "id": "w1wKRddbIY_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ],
      "metadata": {
        "id": "npTYJ8ayR5B3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ],
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to add a custom helpfulness check here!"
      ],
      "metadata": {
        "id": "gC8t-4FISCEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def check_helpfulness(state):\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "  helpfulness_chain = prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    print(\"Helpful!\")\n",
        "    return \"end\"\n",
        "  else:\n",
        "    print(\"Not helpful!\")\n",
        "    return \"continue\"\n",
        "\n",
        "def dummy_node(state):\n",
        "  return"
      ],
      "metadata": {
        "id": "ZV_PxI5zNY7f"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####ðŸ—ï¸ Activity #3:\n",
        "\n",
        "Please write what is happening in our `check_helpfulness` function!\n",
        "\n",
        "#### ANSWER\n",
        "- Extracts the initial user query  and the final response from the assistant from the conversation state\n",
        "- To prevent an excessively long conversation, if the conversation exceeds 10 messages, the function returns \"END\" to terminate the process\n",
        "-Prompt template to determine if the final response is extremely helpful and indicate the helpfulness with gpt4 model\n",
        "- Helpfulness check chain is created by combining the prompt template, the ChatOpenAI model, and the StrOutputParser\n",
        "- invoke method is called on the helpfulness_chain with the initial query and final response as input which executes the chain and returns the model's assessment of the helpfulness\n",
        "- function checks if the helpfulness response contains the letter 'Y'.  In this case, the function prints \"Helpful!\" and returns \"end\" to indicate that the conversatione else returns \"continue\""
      ],
      "metadata": {
        "id": "Fz1u9Vf4SHxJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ],
      "metadata": {
        "id": "sD7EV0HqSQcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "graph_with_helpfulness_check.add_node(\"action\", call_tool)\n",
        "graph_with_helpfulness_check.add_node(\"passthrough\", dummy_node)"
      ],
      "metadata": {
        "id": "6r6XXA5FJbVf"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ],
      "metadata": {
        "id": "HNWHwWxuRiLY"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : \"passthrough\"\n",
        "    }\n",
        ")\n",
        "\n",
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"passthrough\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "aVTKnWMbP_8T"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ],
      "metadata": {
        "id": "cbDK2MbuREgU"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compile and test!"
      ],
      "metadata": {
        "id": "_OWOPweuSiHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ],
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
        "\n",
        "messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "3f5ad543-b2a6-4eb0-ec0e-94ff765f47d4"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Query: Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\n",
            "\n",
            "\n",
            "Tool Call - Name: tavily_search_results_json + Query: {\"query\":\"LoRA in machine learning\"}\n",
            "Tool Response: [{'url': 'https://github.com/microsoft/LoRA', 'content': 'About\\nCode for loralib, an implementation of \"LoRA: Low-Rank Adaptation of Large Language Models\"\\nTopics\\nResources\\nLicense\\nCode of conduct\\nSecurity policy\\nStars\\nWatchers\\nForks\\nReleases\\nContributors\\n14\\nLanguages\\nFooter\\nFooter navigation Latest commit\\nGit stats\\nFiles\\nREADME.md\\nLoRA: Low-Rank Adaptation of Large Language Models\\n(For the radio communication technique, see LoRa.)\\n Code for loralib, an implementation of \"LoRA: Low-Rank Adaptation of Large Language Models\"\\nLicense\\nmicrosoft/LoRA\\nName already in use\\nUse Git or checkout with SVN using the web URL.\\n There are several directories in this repo:\\nQuickstart\\nAdditional Notes\\nWhile we focus on a simple yet effect setup, namely adapting only the q and v projection in a Transformer, in our examples, LoRA can be apply to any subsets of pre-trained weights. Most contributions require you to agree to a\\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\\nthe rights to use your contribution.'}, {'url': 'https://arxiv.org/abs/2106.09685', 'content': 'An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is ...'}, {'url': 'https://towardsdatascience.com/lora-intuitively-and-exhaustively-explained-e944a6bff46b', 'content': 'More from Daniel Warfield and Towards Data Science\\nDaniel Warfield\\nin\\nTowards Data Science\\nTransformers\\u200aâ€”\\u200aIntuitively and Exhaustively Explained\\nExploring the modern wave of machine learning: taking apart the transformer step by step\\n--\\n14\\nRahul Nayak\\nin\\nTowards Data Science\\nHow to Convert Any Text Into a Graph of Concepts\\nA method to convert any text corpus into a Knowledge Graph using Mistral 7B.\\n--\\n35\\nMarco Peixeiro\\nin\\nTowards Data Science\\nTimeGPT: The First Foundation Model for Time Series Forecasting\\nExplore the first generative pre-trained forecasting model and apply it in a project with Python\\n--\\n22\\nDaniel Warfield\\nin\\nTowards Data Science\\nConvolutional Networks\\u200aâ€”\\u200aIntuitively and Exhaustively Explained\\nUnpacking a cornerstone modeling strategy\\n--\\n3\\nRecommended from Medium\\nRahul Nayak\\nin\\nTowards Data Science\\nHow to Convert Any Text Into a Graph of Concepts\\nA method to convert any text corpus into a Knowledge Graph using Mistral 7B.\\n--\\n35\\nAlden Do Rosario\\nLangchain is NOT for production use. From MLOps to mountingâ€¦\\n--\\n20\\nLists\\nPredictive Modeling w/ Python\\nPractical Guides to Machine Learning\\nNatural Language Processing\\nNew_Reading_List\\nPaul Rose\\nI Found A Very Profitable AI Side Hustle\\nAnd itâ€™s perfect for beginners\\n--\\n192\\nDevansh\\nin\\nDataDrivenInvestor\\nWhy Elon Musks AI Model Grok is the future of LLMs\\nWhy Big Tech Companies should copy the Grok approach ASAP\\n--\\n11\\nThe Pareto Investor\\nNASA Just Shut Down Quantum Computer After Something Insane Happened!\\n Houston, We Have a Problem!\\n--\\n136\\nGathnex\\nMistral-7B Fine-Tuning: A Step-by-Step Guide\\nIntroducing Mistral 7B: The Powerhouse of Language Models\\n--\\n6\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams Member-only story\\nNatural Language Processing | Machine Learning\\nLoRA â€” Intuitively and Exhaustively Explained\\nExploring the modern wave of machine learning with cutting edge fine tuning\\nDaniel Warfield\\nFollow\\nTowards Data Science\\n--\\n7\\nShare\\nFine tuning is the process of tailoring a machine learning model to a specific application, which can be vital in achieving consistent and high quality performance. What, and Why, is Fine Tuning?\\n--\\n--\\n7\\nWritten by Daniel Warfield\\nTowards Data Science\\nData Scientist, Educator, Artist, Writer.\\n'}, {'url': 'https://www.datacamp.com/tutorial/mastering-low-rank-adaptation-lora-enhancing-large-language-models-for-efficient-adaptation', 'content': 'Natassha Selvaraj\\n8 min\\nWhat Fortune 1000 Executives Believe about Data & AI in 2024 with Randy Bean, Innovation Fellow, Data Strategy, Wavestone\\nRichie Cotton\\n46 min\\nData Security in the Age of AI with Bart Vandekerckhove, Co-founder at Raito\\nRichie Cotton\\n46 min\\nBuilding Trustworthy AI with Alexandra Ebert, Chief Trust Officer at MOSTLY AI\\nRichie Cotton\\n50 min\\nHow Transformers Work: A Detailed Exploration of Transformer Architecture\\nJosep Ferrer\\n15 min\\nCross-Entropy Loss Function in Machine Learning: Latest news about our products and team\\nDiscover content by tools and technology\\nDiscover content by data science topics\\nMastering Low-Rank Adaptation (LoRA): Enhancing Large Language Models for Efficient Adaptation\\nLoRA: Low-Rank Adaptation of Large Language Models\\nThe field of machine learning and natural language processing (NLP) has witnessed a remarkable advancement with the introduction of Large Language Models (LLMs) such as GPT, LLaMa, Claude 2, etc. Benefits of pre-fix tuning:\\nLoRA and prefix tuning can be combined within the PEFT (Parameter Efficient Fine-Tuning) framework:\\nExample of LoRA implementation using Loralib library in Python\\nTo implement LoRA, you can use the Loralib library by Microsoft. What Fortune 1000 Executives Believe about Data & AI in 2024 with Randy Bean, Innovation Fellow, Data Strategy, Wavestone\\nData Security in the Age of AI with Bart Vandekerckhove, Co-founder at Raito\\nBuilding Trustworthy AI with Alexandra Ebert, Chief Trust Officer at MOSTLY AI\\nHow Transformers Work: A Detailed Exploration of Transformer Architecture\\nCross-Entropy Loss Function in Machine Learning: Advantages of LoRA\\nThere are several advantages that come with using LoRA for fine-tuning:\\n1. Efficiency in training and deployment\\nLoRA reduces the computational burden, allowing faster adaptation of models.'}, {'url': 'https://medium.com/data-science-in-your-pocket/lora-for-fine-tuning-llms-explained-with-codes-and-example-62a7ac5a3578', 'content': 'DBS Bank | Youtube: https://www.youtube.com/channel/UCQoNosQTIxiMTL9C-gvFdjA\\nMore from Mehul Gupta and Data Science in your pocket\\nMehul Gupta\\nin\\nData Science in your pocket\\nRecommendation Systems using Langchain and LLMs with codes\\nusing RAG framework and chains\\n--\\n3\\nMehul Gupta\\nin\\nData Science in your pocket\\nLangchain tutorials for newbies\\nLangchain use cases with demo explained\\n--\\n2\\nMehul Gupta\\nin\\nData Science in your pocket\\nWhat are Vector Databases and How Langchain uses Vector DBs\\nwith codes and examples\\n--\\n1\\nMehul Gupta\\nin\\nData Science in your pocket\\nBest AI-Agents you should know\\nHuggingGPT, DemoGPT, AutoGPT and more\\n--\\nRecommended from Medium\\nBijit Ghosh\\nAdvanced Techniques for Fine-Tuning LLMs\\nIntroduction\\n--\\nGaurav Garg\\nin\\nGoPenAI\\nUnderstanding LLM Fine Tuning\\u200aâ€”\\u200aA Complete Guide for Everyone\\nThe Rise of Large Language Models and Fine Tuning\\n--\\nLists\\nPredictive Modeling w/ Python\\nChatGPT prompts\\nChatGPT\\nAI Regulation\\nDhanoop Karunakaran\\nin\\nIntro to Artificial Intelligence\\nFine-tuning Large Language Models series: Part1\\u200aâ€”\\u200aInternal mechanism of LLMs\\n Akriti Upadhyay\\nEmploying QdrantDB to conduct advanced similarity searches for image data\\nImplementing Image Similarity Search using Qdrant\\n--\\n1\\nAbhinav Kimothi\\nin\\nMLearning.ai\\nWhat is a fine-tuned LLM?\\nFine-tuning large language models (LLMs) has become a powerful technique for achieving impressive performance in various natural languageâ€¦\\n--\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams Have a look below\\nWe will start off with pip installing libraries\\nNext, import the important functions required\\nNext, letâ€™s load the training and test dataset alongside the LLM to be fine-tuned with its tokenizer\\nNext, using the Input and Output, we will create a prompt template which is a requirement by the SFTTrainer we will be using later\\nNow is the time we set the trainer for LoRA\\n LoRA for Fine-Tuning LLMs explained with codes and example\\nHow to fine-tune your LLMs faster using LoRA\\nMehul Gupta\\nFollow\\nData Science in your pocket\\n--\\nListen\\nShare\\nThis entire year in AI space has been revolutionary because of the advancements in Gen-AI especially the incoming of LLMs. Once done, login into huggingface-hub using the WRITE token in the Jupyter Notebook and push your model using the below code\\nNow, upload the model\\nNow, once uploaded, again use the notebook login but using the READ token this time\\nNow run the below code for inferencing with your private model\\nSee the output for yourself\\nWith this, we will be wrapping up this very long post.'}]\n",
            "\n",
            "Tool Call - Name: tavily_search_results_json + Query: {\"query\":\"Tim Dettmers\"}\n",
            "Tool Response: [{'url': 'https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/', 'content': 'Now I have a simple question: should I use the integrated graphics on CPU to connect with the monitor for display purposes?\\nDoes connecting the graphics card with a dual-monitor in QHD affect the performance of the graphics card during training?\\nThanks, Shuhao\\nTim Dettmers says\\n2020-11-06 at 15:34\\nUsually, the displays do not need that much memory. I want to thank Agrin Hilmkil, Ari Holtzman, Gabriel Ilharco, Nam Pho for their excellent feedback on the previous version of this blog post.\\nRelated\\nRelated Posts\\nFiled Under: Deep Learning, Hardware Tagged With: AMD, CPU, High Performance Computing, Matrix Multiplication, Parallel Computing, PCIe Lanes, Sparse Training\\nReader Interactions\\nComments\\nZoran says\\n2023-04-30 at 08:21\\nHello,\\nHave you had any chance to test high end CPU only interface, for example on Intel 13900K CPU? I am looking to buy a GPU to use for deep learning (Computer vision, objection detection & NLP using neural networks â€“ RCNN, FCN, Yolo , SSD, CTPN, EASTâ€¦etc) I am a bit tight in budget, I was looking at the 1060 (6GB) as a minimum, how does it compare to the 1660 for the same purpose?\\nread in one of your comments above that 1660 does not ave tensor cores and maybe good for gaming but not for deep learningâ€¦ which one do you suggest? For past updates of this blog post, I want to thank Mat Kelcey for helping me to debug and test custom code for the GTX 970; I want to thank Sander Dieleman for making me aware of the shortcomings of my GPU memory advice for convolutional nets; I want to thank Hannes Bretschneider for pointing out software dependency problems for the GTX 580; and I want to thank Oliver Griesel for pointing out notebook solutions for AWS instances. In your Quora article, you wrote:\\nâ€œHowever, if you now use a fleet of either Ferraris and big trucks (thread parallelism), and you have a big job with many packages (large chunks of memory such as matrices) then you will wait for the first truck a bit, but after that you will have no waiting time at all â€” unloading the packages takes so much time that all the trucks will queue in unloading location B so that you always have direct access to your packages (memory).'}, {'url': 'https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/', 'content': 'Dear Tim Dettmers, Thank you very much for this blog. This information is really useful for upcoming deep learning project. In my work place we are developing a server kind on of system to run three deep learning projects. To run all the models in a concurrent manner for 24Ã—7, I need nearly 100 GB GTX 2080 Ti GPU. To maintain this GPU what ...'}, {'url': 'https://scholar.google.com/citations?user=lHI3w5kAAAAJ', 'content': 'Tim Dettmers. University of Washington. Verified email at cs.washington.edu - Homepage. Deep Learning Natural Language Processing. Articles Cited by Public access Co-authors. Title. ... A Borzunov, M Ryabinin, T Dettmers, Q Lhoest, L Saulnier, M Diskin, ... NeurIPS 2021 Demonstration, 2022. 9: 2022:'}, {'url': 'https://timdettmers.com/', 'content': 'Tim Dettmers is a researcher and teacher in deep learning and natural language processing. His blog covers topics such as GPU choice, sparse training, creativity, and grad school applications.'}, {'url': 'https://timdettmers.com/about/', 'content': 'Tim Dettmers, Luke Zettlemoyer. [bib] 2018. Convolutional 2D Knowledge Graph Embeddings, Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel. AAAI2018. 2016. 8-Bit Approximations for Parallelism in Deep Learning, Tim Dettmers. ICLR2016.'}]\n",
            "\n",
            "Tool Call - Name: tavily_search_results_json + Query: {\"query\":\"Attention in machine learning\"}\n",
            "Tool Response: [{'url': 'https://machinelearningmastery.com/the-attention-mechanism-from-scratch/', 'content': 'The General Attention Mechanism\\nThe general attention mechanism makes use of three main components, namely the queries, $\\\\mathbf{Q}$, the keys, $\\\\mathbf{K}$, and the values, $\\\\mathbf{V}$.\\nIf you had to compare these three components to the attention mechanism as proposed by Bahdanau et al., then the query would be analogous to the previous decoder output, $\\\\mathbf{s}_{t-1}$, while the values would be analogous to the encoded inputs, $\\\\mathbf{h}_i$. In the Bahdanau attention mechanism, the keys and values are the same vector.\\n But if we have thought for example the first row of the score matrix as:\\n[ word1/word1 word1/word2 word1/word3 word1/word4 ]\\nthen the elements of the attention vector (a row of the attention matrix) might be interpreted as somewhat a coded version of the row of the score matrix since in the product between two matrix we do rows*columns product.\\n In the latter case, if I get, for example, word 3, then the first element of the corresponding attention vector (row) is related to the first word, the second element to the second, and the third element to the fourth word ?.\\n Note\\xa0that Bahdanau et al.â€™s attention mechanism is divided into the step-by-step computations of the alignment scores, the weights, and the context vector:\\n$$e_{t,i} = a(\\\\mathbf{s}_{t-1}, \\\\mathbf{h}_i)$$\\n$$\\\\alpha_{t,i} = \\\\text{softmax}(e_{t,i})$$\\n$$\\\\mathbf{c}_t = \\\\sum_{i=1}^T \\\\alpha_{t,i} \\\\mathbf{h}_i$$\\nBahdanau et al. implemented an RNN for both the encoder and decoder.\\n The idea behind the attention mechanism was to permit the decoder to utilize the most relevant parts of the input sequence in a flexible manner, by a weighted combination of all the encoded input vectors, with the most relevant vectors being attributed the highest weights.\\n'}, {'url': 'https://en.wikipedia.org/wiki/Attention_(machine_learning)', 'content': 'Note that the context vector for \"that\" does not rely on context vectors for the other words; therefore the context vectors of all words can be calculated using the whole matrix X, which includes all the word embeddings, instead of a single word\\'s embedding vector x in the formula above, thus parallelizing the calculations. More correctly, we should take the transpose of the context vector and use the column-wise softmax, resulting in the more correct form\\nThe query vector is compared (via dot product) with each word in the keys. The structure of the input data is captured in the Qw and Kw weights, and the Vw weights express that structure in terms of more meaningful features for the task being trained for.\\n On the first pass through the decoder, 94% of the attention weight is on the first English word \"I\", so the network offers the word \"je\".\\n Multiplying this against the value matrix effectively amplifies the signal for the most important words in the sentence and diminishes the signal for less important words.[9]\\n'}, {'url': 'https://machinelearningmastery.com/what-is-attention/', 'content': 'The human brain does so by relying on attention, such that it dynamically stores in memory the information that the human subject most pays attention to.\\nAttention in Machine Learning\\nImplementing the attention mechanism in artificial neural networks does not necessarily track the biological and psychological mechanisms of the human brain. A list of these vectors (the second component of the attention-based system above), together with the decoderâ€™s previous hidden states, will be exploited by the attention mechanism to dynamically highlight which of the input information will be used to generate the output.\\n LinkedIn |\\nTwitter |\\nFacebook |\\nNewsletter |\\nRSS\\nPrivacy |\\nDisclaimer |\\nTerms |\\nContact |\\nSitemap |\\nSearch Instead, it is the ability to dynamically highlight and use the salient parts of the information at handâ€”in a similar manner as it does in the human brainâ€”that makes attention such an attractive concept in machine learning.\\n At each time step, the attention mechanism then takes the previous hidden state of the decoder and the list of encoded vectors, using them to generate unnormalized score values that indicate how well the elements of the input sequence align with the current output.'}, {'url': 'https://www.geeksforgeeks.org/ml-attention-mechanism/', 'content': 'We can observe 3 sub-parts or components of the Attention Mechanism architecture :\\nConsider the following Encoder-Decoder architecture with Attention.\\nEncoder-Decoder with Attention\\nEncoder:\\nThe encoder applies recurrent neural networks (RNNs) or transformer-based models to iteratively process the input sequence. Data Structures and Algorithms\\nML & Data Science\\nWeb Development\\nLanguages\\nInterview Corner\\nCS Subjects\\nJobs\\nPractice\\nContests\\nML â€“ Attention mechanism\\nLetâ€™s take a look at hearing and a case study of selective attention in the context of a crowded cocktail party. The context vector is fed into the decoder along with the current hidden state of the decoder in order to predict the next token in the output sequence. The attention mechanism can be represented mathematically as follows:\\nattention\\nThe feed-forward network is responsible for transforming the target hidden state into a representation that is compatible with the attention mechanism. This architecture allows the model to focus on different parts of the input sequence during the translation process, improving the alignment and quality of the translations.'}, {'url': 'https://towardsdatascience.com/introduction-to-attention-mechanism-8d044442a29', 'content': \"Thatâ€™s why youâ€™re not using just a dot product, but a scaled dot product, that way our new formula looks like\\nIf youâ€™re a having problem understanding why dot product creates a large number with high dimensional vectors please check 3Blue1Brownâ€™s Youtube video on the subject\\nAdditionally, we want to be able to use more than one query vector q. It was great to have a single query vector for each timestamp of the decoder but it can be a lot simpler when we use all of them at the same time, so we change our vector to vectors Q (Shape NQ\\u200bÃ—DQ\\u200b). References:\\nOriginally published at https://erdem.pl.\\n--\\n--\\nWritten by Kemal Erdem (burnpiro)\\nTowards Data Science\\nML Engineer, Javascript Architect, Consultant, MTB lover\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams The reason was to match the colors used in The Illustrated Transformer blog post.\\nConclusions\\nFrom this point onwards we can use the Self-Attention Layer to create a Transformer but this article is too long already. In the case of the token woman, both people on the image save similar attention weights but that's still ok because the model could decide which one is the subject and how to name that person.\\n Now we have to pass the same grid and s0\\u200b to the alignment function to calculate the corresponding alignment score for each value of the grid\\nThat gives us alignment scores for t=1 timestep.\"}]\n",
            "\n",
            "Agent Response: ### LoRA in Machine Learning:\n",
            "LoRA stands for \"Low-Rank Adaptation of Large Language Models.\" It is an adaptation technique for large-scale pre-training in natural language processing. LoRA focuses on adapting specific subsets of pre-trained weights, such as q and v projections in a Transformer model. It aims to reduce the computational burden and enable faster adaptation of models. You can find more information about LoRA implementation in the [LoRA GitHub repository](https://github.com/microsoft/LoRA).\n",
            "\n",
            "### Tim Dettmers:\n",
            "Tim Dettmers is a researcher and teacher specializing in deep learning and natural language processing. He covers various topics related to GPU choice, sparse training, creativity, and grad school applications on his [blog](https://timdettmers.com/). Tim Dettmers has contributed to research in areas such as convolutional 2D knowledge graph embeddings and 8-bit approximations for parallelism in deep learning.\n",
            "\n",
            "### Attention in Machine Learning:\n",
            "Attention in machine learning refers to a mechanism that dynamically highlights and uses the salient parts of information during model training. It allows models to focus on relevant parts of the input sequence, improving performance and alignment in tasks like translation and natural language processing. The attention mechanism involves components like queries, keys, and values to compute alignment scores and context vectors. You can learn more about the attention mechanism from resources like [Machine Learning Mastery](https://machinelearningmastery.com/what-is-attention/) and [Towards Data Science](https://towardsdatascience.com/introduction-to-attention-mechanism-8d044442a29).\n",
            "\n",
            "If you need more detailed information on any of these topics, feel free to ask!\n"
          ]
        }
      ]
    }
  ]
}